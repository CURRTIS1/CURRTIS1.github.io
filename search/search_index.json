{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Github Pages Documentation","text":"<p>This is a Github Pages to store my project documentation on:</p> <ul> <li>AWS</li> <li>Python</li> <li>Terraform</li> <li>Pipelines</li> <li>Docker</li> <li>Kubernetes</li> <li>Any other projects</li> </ul>"},{"location":"professional-summary/","title":"Professional Profile","text":""},{"location":"professional-summary/#profile","title":"Profile","text":"<p>I am well organised which transfers from my home life to work. I've always been driven and have a keen interest in technologies.</p>"},{"location":"professional-summary/#experience","title":"Experience","text":""},{"location":"professional-summary/#rackspace-technology","title":"Rackspace Technology","text":""},{"location":"professional-summary/#aws-cloud-practice-engineer-iii-2021-current","title":"AWS Cloud Practice Engineer III (2021 - Current)","text":"<p>My role is to work within sprints planning and implementing project work with our customers. My technical focuses are:</p> <ul> <li>Deploying infrastructure using Terraform within a pipeline</li> <li>Design and deploy well-architected environments</li> <li>Backup, Security and Cost management</li> <li>Containers (ECS/EKS)</li> <li>Write scripts (Boto3/Bash) for reusability</li> </ul>"},{"location":"professional-summary/#aws-cloud-engineer-ii-2020-2021","title":"AWS Cloud Engineer II (2020 \u2013 2021)","text":"<p>My role as an AWS Cloud engineer was to build, manage and support customer cloud environments with a strict SLA. With a focus on Infrastructure as code we utilise terraform and cloudformation to design and build customer solutions. My technical focuses were:</p> <ul> <li>Supporting a broad list of AWS core services for customer solutions</li> <li>IAC (terraform and cloudformation)</li> <li>Usage of git and git workflows</li> <li>Bash/powershell/python</li> <li>Containerisation (ecs,eks)</li> <li>Solid understanding of the overall Cloud infrastructure (EC2, RDS, S3, VPC, etc)</li> <li>Knowledge of both Linux and Windows OS</li> </ul>"},{"location":"professional-summary/#windows-systems-administrator-i-2017-2020","title":"Windows Systems Administrator I (2017 \u2013 2020)","text":"<p>My role was to manage and support customer solutions in a timely manner. Working within an agreed SLA we are required to troubleshoot and resolve issues under pressure. My technical focuses were:</p> <ul> <li>Supporting Windows Server OS: 2008 (R2), 2012 (R2), 2016 Architecture</li> <li>Supporting of SQL 2012 through to 2016</li> <li>Supporting clusters - MSSQL / MSDTC / File Clusters</li> <li>Use of VMWare (VMotions, SVMotions, General administration)</li> <li>Troubleshooting Active Directory, DNS, Domain registration</li> <li>Troubleshooting IIS, FTP, DFSR</li> </ul>"},{"location":"professional-summary/#windows-implementation-engineer-2015-2017","title":"Windows Implementation Engineer (2015 - 2017)","text":"<p>My role was to deploy customer solutions in a timely manner. My technical focuses were:</p> <ul> <li>Implementing Windows Server OS: 2008 (R2), 2012 (R2), 2016 Architecture</li> <li>Installation of SQL through automation - SQL 2012 through to 2016</li> <li>Implementing high availability solutions, deploying Clusters - MSSQL / MSDTC / File Clusters</li> <li>Use of Virtualisation (VMWare) and physical hardware (Raid, DRAC, ILO)</li> <li>An understanding of reading and writing Powershell scripts that would assist with deployments</li> <li>Building out Active Directory environments with an understanding of DNS</li> </ul>"},{"location":"professional-summary/#hotels4ucomthomas-cook","title":"Hotels4u.com/Thomas Cook","text":""},{"location":"professional-summary/#infrastructure-engineer-2012-2015","title":"Infrastructure Engineer (2012-2015)","text":"<p>My role was to manage the external facing infrastructure. Maintaining site availability, uptime and code releases. My technical focuses were:</p> <ul> <li>Full support and administration of the production environment</li> <li>Performance monitoring and Incident management</li> <li>Prepare capacity planning and sizing</li> <li>Plan, implement and maintain security</li> <li>Centralised error log management</li> <li>Perform release deployments using manual and automate methods</li> </ul>"},{"location":"professional-summary/#education","title":"Education","text":"<p>Sept 2009 \u2013 June 2012: University of Sheffield, BSc Mathematics, Second class Hons. Sept 2002 \u2013 July 2009: St Clement Danes School: A Level: Mathematics (A), Business Studies (B), Graphic Design Grade (B).</p>"},{"location":"professional-summary/#professional-certifications","title":"Professional Certifications","text":""},{"location":"professional-summary/#volunteering-experience","title":"Volunteering Experience","text":""},{"location":"professional-summary/#samaritans-2019-current","title":"Samaritans 2019 - Current","text":"<ul> <li>Listening Volunteer</li> <li>Technical assistance</li> </ul>"},{"location":"AWS/how-to-deploy-using-java-cdk/","title":"How to deploy using AWS Java CDK","text":"<p>Note</p> <p>The code for this is located in <code>technical-documentation/AWS/AWS_Java_CDK/hello-cdk</code></p>"},{"location":"AWS/how-to-deploy-using-java-cdk/#prerequisites","title":"Prerequisites","text":"<p>You must have Java and Maven installed, we do this inside the Dev Container with:</p> <pre><code># Install NVM\nRUN nvm install ${NODE_VERSION}\n\n# Install AWSCLI\nRUN pip install --upgrade pip &amp;&amp; \\\n    pip install --upgrade awscli\n\n# Install CDK\nRUN npm install -g aws-cdk\nRUN cdk --version\n\n# Install Maven\nRUN sudo apt-get install maven -y\n</code></pre>"},{"location":"AWS/how-to-deploy-using-java-cdk/#installation","title":"Installation","text":"<p>Install CDK Toolkit if this isn't already done in your Dev Container</p> <pre><code>npm install -g aws-cdk \n</code></pre> <p>You may need to install NPM</p> <pre><code>sudo apt-get install npm\n</code></pre>"},{"location":"AWS/how-to-deploy-using-java-cdk/#set-your-aws-credential-environment-variables","title":"Set your AWS credential environment variables","text":"<p>Run the below:</p> <pre><code>export AWS_ACCESS_KEY_ID=ABCDEFGHIJKLMNOPQRSTUVWXYZ\nexport AWS_SECRET_ACCESS_KEY=ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre>"},{"location":"AWS/how-to-deploy-using-java-cdk/#building-your-application","title":"Building your application","text":"<p>Build your app directory:</p> <pre><code>mkdir hello-cdk\ncd hello-cdk\n</code></pre> <p>Now initialize the app by using the cdk init command. Specify the desired template (\"app\") and programming language as shown in the following examples:</p> <pre><code>cdk init app --language java\n</code></pre> <p>Build the app:</p> <pre><code>mvn compile -q\n</code></pre> <p>You may need to run a maven dependency:</p> <pre><code>mvn dependency:list\n</code></pre> <p>If you get the error below make sure to bootstrap:</p> <p><code>current credentials could not be used to assume 'arn:aws:iam::{ACCOUNT_ID}:role/cdk-hnb659fds-deploy-role-024830515316-{REGION}', but are for the right account. Proceeding anyway.</code></p> <pre><code>cdk bootstrap\n</code></pre> <p>Note</p> <p>If you're deploying this again to a new account such as the ACG playground, you'd need to bootstrap again.</p>"},{"location":"AWS/how-to-deploy-using-java-cdk/#add-a-resources","title":"Add a resources","text":"<p>Adding an S3 bucket, add the below to <code>src/main/java/com/myorg/HelloCdkStack.java</code></p> <pre><code>package com.myorg;\n\nimport software.amazon.awscdk.*;\nimport software.amazon.awscdk.services.s3.Bucket;\n\npublic class HelloCdkStack extends Stack {\n    public HelloCdkStack(final App scope, final String id) {\n        this(scope, id, null);\n    }\n\n    public HelloCdkStack(final App scope, final String id, final StackProps props) {\n        super(scope, id, props);\n\n        Bucket.Builder.create(this, \"MyFirstBucket\")\n            .versioned(true).build();\n    }\n}\n</code></pre>"},{"location":"AWS/how-to-deploy-using-java-cdk/#deploy-application","title":"Deploy application","text":"<p>Synthesize an AWS CloudFormation template for the app, as follows.</p> <pre><code>cdk synth\n</code></pre> <p>Note</p> <p>The cdk synth command executes your app, which causes the resources defined in it to be translated into an AWS CloudFormation template. The displayed output of cdk synth is a YAML-format template. Following, you can see the beginning of our app's output. The template is also saved in the cdk.out directory in JSON format.</p> <p>To deploy the stack using AWS CloudFormation, issue:</p> <pre><code>cdk deploy\n</code></pre> <p>Note</p> <p>As with cdk synth, you don't need to specify the name of the stack if there's only one in the app. It is optional (though good practice) to synthesize before deploying. The AWS CDK synthesizes your stack before each deployment.</p> <p>You will see the below if the deployment is successful.</p> <p></p>"},{"location":"AWS/how-to-deploy-using-java-cdk/#deploying-an-application-written-in-cloudformation","title":"Deploying an application written in Cloudformation","text":"<p>Run the above steps up until:</p> <pre><code>cdk bootstrap\n</code></pre> <p>Add the cloudformation template to <code>src/main/java/com/myorg/</code>.</p> <p>Adding an S3 bucket, add the below to <code>src/main/java/com/myorg/HelloCfnCdkApp.java</code> referencing your template file <code>s3.template</code>.</p> <p>The <code>s3.template</code> should be located in the app root, the same folder as pom.xml.</p> <pre><code>import software.amazon.awscdk.Stack;\nimport software.amazon.awscdk.StackProps;\nimport software.amazon.awscdk.cloudformation.include.CfnInclude;\nimport software.constructs.Construct;\n\npublic class HelloCfnCdkApp extends Stack {\n    public MyStack(final Construct scope, final String id) {\n        this(scope, id, null);\n    }\n\n    public HelloCfnCdkApp(final Construct scope, final String id, final StackProps props) {\n        super(scope, id, props);\n\n        CfnInclude template = CfnInclude.Builder.create(this, \"Template\")\n            .templateFile(\"s3.template\")\n            .build();\n    }\n}\n</code></pre> <p>To deploy the stack using AWS CloudFormation, issue:</p> <pre><code>cdk deploy\n</code></pre> <p>To deploy the stack using AWS CloudFormation and a parameter, issue:</p> <pre><code>cdk deploy MyStack --parameters uploadBucketName=uploadbucket\n</code></pre> <p>To reference multiple paramters:</p> <pre><code>cdk deploy MyStack --parameters uploadBucketName=upbucket --parameters downloadBucketName=downbucket\n</code></pre> <p>Note</p> <p>By default, the AWS CDK retains values of parameters from previous deployments and uses them in subsequent deployments if they are not specified explicitly. Use the --no-previous-parameters flag to require all parameters to be specified.</p>"},{"location":"AWS/how-to-deploy-using-java-cdk/#reference-links","title":"Reference Links","text":"<p>\"Getting started with the AWS CDK\"</p> <p>\"Your first AWS CDK app\"</p> <p>\"CDK Parameters\"</p>"},{"location":"AWS/how-to-deploy-using-python-cdk/","title":"How to deploy using AWS Python CDK","text":"<p>Note</p> <p>The code for this is located in <code>technical-documentation/AWS/AWS_Python_CDK/hello-cdk/</code></p>"},{"location":"AWS/how-to-deploy-using-python-cdk/#prerequisites","title":"Prerequisites","text":"<p>You must have  installed, we do this inside the Dev Container with:</p> <pre><code># Install NVM\nRUN nvm install ${NODE_VERSION}\n\n# Install AWSCLI\nRUN pip install --upgrade pip &amp;&amp; \\\n    pip install --upgrade awscli\n\n# Install CDK\nRUN npm install -g aws-cdk\nRUN cdk --version\n\n# Install Maven\nRUN sudo apt-get install maven -y\n</code></pre>"},{"location":"AWS/how-to-deploy-using-python-cdk/#installation","title":"Installation","text":"<p>Install CDK Toolkit if this isn't already done in your Dev Container</p> <pre><code>npm install -g aws-cdk \n</code></pre> <p>You may need to install NPM</p> <pre><code>sudo apt-get install npm\n</code></pre>"},{"location":"AWS/how-to-deploy-using-python-cdk/#set-your-aws-credential-environment-variables","title":"Set your AWS credential environment variables","text":"<p>Run the below:</p> <pre><code>export AWS_ACCESS_KEY_ID=ABCDEFGHIJKLMNOPQRSTUVWXYZ\nexport AWS_SECRET_ACCESS_KEY=ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre>"},{"location":"AWS/how-to-deploy-using-python-cdk/#building-your-application","title":"Building your application","text":"<p>Build your app directory:</p> <pre><code>mkdir hello-cdk\ncd hello-cdk\n</code></pre> <p>Now initialize the app by using the cdk init command. Specify the desired template (\"app\") and programming language as shown in the following examples:</p> <pre><code>cdk init app --language java\n</code></pre> <p>Build the app:</p> <pre><code>mvn compile -q\n</code></pre> <p>You may need to run a maven dependency:</p> <pre><code>mvn dependency:list\n</code></pre> <p>If you get the error below make sure to bootstrap:</p> <p><code>current credentials could not be used to assume 'arn:aws:iam::{ACCOUNT_ID}:role/cdk-hnb659fds-deploy-role-024830515316-{REGION}', but are for the right account. Proceeding anyway.</code></p> <pre><code>cdk bootstrap\n</code></pre> <p>Note</p> <p>If you're deploying this again to a new account such as the ACG playground, you'd need to bootstrap again.</p>"},{"location":"AWS/how-to-deploy-using-python-cdk/#add-a-resources","title":"Add a resources","text":"<p>Adding an S3 bucket, add the below to <code>src/main/java/com/myorg/HelloCdkStack.java</code></p> <pre><code>package com.myorg;\n\nimport software.amazon.awscdk.*;\nimport software.amazon.awscdk.services.s3.Bucket;\n\npublic class HelloCdkStack extends Stack {\n    public HelloCdkStack(final App scope, final String id) {\n        this(scope, id, null);\n    }\n\n    public HelloCdkStack(final App scope, final String id, final StackProps props) {\n        super(scope, id, props);\n\n        Bucket.Builder.create(this, \"MyFirstBucket\")\n            .versioned(true).build();\n    }\n}\n</code></pre>"},{"location":"AWS/how-to-deploy-using-python-cdk/#deploy-application","title":"Deploy application","text":"<p>Synthesize an AWS CloudFormation template for the app, as follows.</p> <pre><code>cdk synth\n</code></pre> <p>Note</p> <p>The cdk synth command executes your app, which causes the resources defined in it to be translated into an AWS CloudFormation template. The displayed output of cdk synth is a YAML-format template. Following, you can see the beginning of our app's output. The template is also saved in the cdk.out directory in JSON format.</p> <p>To deploy the stack using AWS CloudFormation, issue:</p> <pre><code>cdk deploy\n</code></pre> <p>Note</p> <p>As with cdk synth, you don't need to specify the name of the stack if there's only one in the app. It is optional (though good practice) to synthesize before deploying. The AWS CDK synthesizes your stack before each deployment.</p> <p>You will see the below if the deployment is successful.</p> <p></p>"},{"location":"AWS/how-to-deploy-using-python-cdk/#deploying-an-application-written-in-cloudformation","title":"Deploying an application written in Cloudformation","text":"<p>Run the above steps up until:</p> <pre><code>cdk bootstrap\n</code></pre> <p>Add the cloudformation template to <code>src/main/java/com/myorg/</code>.</p> <p>Adding an S3 bucket, add the below to <code>src/main/java/com/myorg/HelloCfnCdkApp.java</code> referencing your template file <code>s3.template</code>.</p> <p>The <code>s3.template</code> should be located in the app root, the same folder as pom.xml.</p> <pre><code>import software.amazon.awscdk.Stack;\nimport software.amazon.awscdk.StackProps;\nimport software.amazon.awscdk.cloudformation.include.CfnInclude;\nimport software.constructs.Construct;\n\npublic class HelloCfnCdkApp extends Stack {\n    public MyStack(final Construct scope, final String id) {\n        this(scope, id, null);\n    }\n\n    public HelloCfnCdkApp(final Construct scope, final String id, final StackProps props) {\n        super(scope, id, props);\n\n        CfnInclude template = CfnInclude.Builder.create(this, \"Template\")\n            .templateFile(\"s3.template\")\n            .build();\n    }\n}\n</code></pre> <p>To deploy the stack using AWS CloudFormation, issue:</p> <pre><code>cdk deploy\n</code></pre> <p>To deploy the stack using AWS CloudFormation and a parameter, issue:</p> <pre><code>cdk deploy MyStack --parameters uploadBucketName=uploadbucket\n</code></pre> <p>To reference multiple paramters:</p> <pre><code>cdk deploy MyStack --parameters uploadBucketName=upbucket --parameters downloadBucketName=downbucket\n</code></pre> <p>Note</p> <p>By default, the AWS CDK retains values of parameters from previous deployments and uses them in subsequent deployments if they are not specified explicitly. Use the --no-previous-parameters flag to require all parameters to be specified.</p>"},{"location":"AWS/how-to-deploy-using-python-cdk/#reference-links","title":"Reference Links","text":"<p>\"Getting started with the AWS CDK\"</p> <p>\"Your first AWS CDK app\"</p> <p>\"CDK Parameters\"</p>"},{"location":"Git/comparing-github-actions-bitbucket-pipelines/","title":"Comparing Github Actions Bitbucket Pipelines","text":""},{"location":"Git/comparing-github-actions-bitbucket-pipelines/#github-actions-overview","title":"Github Actions Overview","text":"<p>Github actions looks for a <code>.yml</code> or a <code>.yaml</code> file under the <code>.github/workflows</code> directory.</p> <p>GitHub Actions supports Linux, macOS, and Windows Server.</p> <p>Free tier: GitHub Actions free tier gives you 2000 minutes/month and 500 MB of storage.</p>"},{"location":"Git/comparing-github-actions-bitbucket-pipelines/#github-actions-runner","title":"Github Actions Runner","text":"<p>If you use a GitHub-hosted runner, each job runs in a fresh instance of a runner image specified by <code>runs-on</code>.</p> <p>Available GitHub-hosted runner labels are:</p> OS (YAML workflow label) Notes ubuntu-latest, ubuntu-22.04, ubuntu-20.04 The ubuntu-latest label currently uses the Ubuntu 22.04 runner image. windows-latest, windows-2022, windows-2019 The windows-latest label currently uses the Windows 2022 runner image. macos-latest, macos-14 [Beta], macos-13, macos-12, macos-11 The macos-latest workflow label currently uses the macOS 12 runner image. <p>GitHub has an \"Actions Marketplace\" with thousands of actions ready to use. These actions allow us to improve our workflow with things like installing language environments, caching data between jobs, or deploying a project with just a couple of lines of code.</p>"},{"location":"Git/comparing-github-actions-bitbucket-pipelines/#github-actions-artifacts","title":"Github Actions Artifacts","text":"<p>Artifacts allows persistent data after a job has completed, and share that data with another job in the same workflow. An artifact is a file or collection of files produced during a workflow run. For example, you can use artifacts to save your build and test output after a workflow run has ended. All actions and workflows called within a run have write access to that run's artifacts.</p> <p>Using the <code>actions/upload-artifact@v3</code> and <code>actions/download-artifact@v3</code> actions you can have files persist between different jobs, for example to upload a terraform plan named <code>tfplan</code> between one job to another:</p> <pre><code>- name: Publish Terraform Plan\nuses: actions/upload-artifact@v3\nwith:\n    name: tfplan\n    path: ./tfplan\n</code></pre> <pre><code>- name: Download Terraform Plan\nuses: actions/download-artifact@v3\nwith:\n    path: ./tfplan\n    name: tfplan\n</code></pre>"},{"location":"Git/comparing-github-actions-bitbucket-pipelines/#github-actions-secrets","title":"Github Actions Secrets","text":"<p>Under Settings &gt; Secrets and Variables, Actions you can save <code>Repository secrets</code> which can be referenced in your pipeline for example the below pipeline sets AWS access keys using the secrets values which allows Terraform to access your AWS environment.</p> <pre><code>env:\n  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n</code></pre> <p>Note</p> <p>If a secret was used in the job, GitHub automatically redacts secrets printed to the log. You should avoid printing secrets to the log intentionally.</p>"},{"location":"Git/comparing-github-actions-bitbucket-pipelines/#github-actions-example","title":"Github Actions Example","text":"<p>The below is a Github Pipelines example which on push to master branch:</p> <ul> <li>Stores AWS credentials as environment variables</li> <li>Sets up Terraform using a specific version <code>1.5.5</code></li> <li>Runs a terraform init and format</li> <li>Runs a terraform plan and outputs to console on failure</li> <li>Uploads the terraform plan file to artifacts</li> <li>Downloads the artifacts</li> <li>Runs a terraform apply using the plan output from the previous step</li> </ul> <p></p> <pre><code>name: Deploy Terraform\ndefaults:\n  run:\n    shell: bash\n\non:\n  push:\n    branches:\n      - master\n  workflow_dispatch:\n\nenv:\n  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n\njobs:\n  tf_fmt_plan_000base:\n    name: Terraform Format and Plan 000Base\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: ./layers/000base\n    permissions:\n      contents: read\n      pull-requests: write\n    steps:\n      - name: Checkout the repository to the runner\n        uses: actions/checkout@v2\n      - name: Setup Terraform with specified version on the runner\n        uses: hashicorp/setup-terraform@v3\n        with:\n          terraform_version: 1.5.5\n          terraform_wrapper: true\n      - name: Terraform init\n        id: init\n        run: terraform init\n      - name: Terraform format\n        id: fmt\n        run: terraform fmt -check\n        continue-on-error: true\n      - name: Terraform Plan\n        id: plan\n        run: |\n          export exitcode=0\n          terraform plan -detailed-exitcode -no-color -out 000base_tfplan || export exitcode=$?\n\n          echo \"exitcode=$exitcode\" &gt;&gt; $GITHUB_OUTPUT\n\n          if [ $exitcode -eq 1 ]; then\n            echo Terraform Plan Failed!\n            exit 1\n          else \n            exit 0\n          fi\n      - name: Publish Terraform Plan\n        uses: actions/upload-artifact@v3\n        with:\n          name: 000base_tfplan\n          path: ./layers/000base/000base_tfplan\n  tf_apply_000base:\n    name: Terraform Apply 000Base\n    timeout-minutes: 30\n    needs: tf_fmt_plan_000base\n    runs-on: ubuntu-latest\n    environment: 'prod'\n    defaults:\n      run:\n        working-directory: ./layers/000base\n    permissions:\n      contents: read\n      pull-requests: write\n    steps:\n      - name: Checkout the repository to the runner\n        uses: actions/checkout@v2\n      - name: Setup Terraform with specified version on the runner\n        uses: hashicorp/setup-terraform@v3\n        with:\n          terraform_version: 1.5.5\n      - name: Terraform init\n        id: init\n        run: terraform init\n      - name: Download Terraform Plan\n        uses: actions/download-artifact@v3\n        with:\n          path: ./layers/000base/\n          name: 000base_tfplan\n      - name: Terraform Apply\n        run: terraform apply -auto-approve 000base_tfplan\n</code></pre>"},{"location":"Git/comparing-github-actions-bitbucket-pipelines/#bitbucket-pipelines-overview","title":"Bitbucket Pipelines Overview","text":"<p>Bitbucket Pipelines looks for a <code>.yml</code> or a <code>.yaml</code> file called <code>bitbucket-pipelines.yml</code> located at the root directory of the repository.</p> <p>Bitbucket Pipelines runs your builds in Docker containers. These containers run a Docker image that defines the build environment. You can use the default image provided by Bitbucket or get a custom one.</p> <p>Free tier: Bitbucket Pipelines free tier has 50 build minutes/month and 1 GB of storage.</p>"},{"location":"Git/comparing-github-actions-bitbucket-pipelines/#bitbucket-pipelines-runner","title":"Bitbucket Pipelines Runner","text":"<p>Bitbucket Pipelines supports public and private Docker images including those hosted on Docker Hub, AWS, GCP, Azure and self-hosted registries accessible on the internet. (Bitbucket Pipelines cannot currently access Docker images that cannot be accessed via the internet.)</p> <p>Note</p> <p>Bitbucket Pipelines only supports Linux environments at the moment.</p>"},{"location":"Git/comparing-github-actions-bitbucket-pipelines/#bitbucket-pipelines-artifacts","title":"Bitbucket Pipelines Artifacts","text":"<p>Artifacts allows persistent data after a job has completed, and share that data with another job in the same workflow. An artifact is a file or collection of files produced during a workflow run. For example, you can use artifacts to save your build and test output after a workflow run has ended. All actions and workflows called within a run have write access to that run's artifacts.</p> <p>Using the <code>artifacts</code> option in a step allows you to have files persist between different steps, for example to upload a terraform plan named <code>tfplan</code> between one job to another:</p> <pre><code>- step:\n    name: terraformplan\n    script:\n        - terraform plan -no-color -out tfplan\n    artifacts:\n        - ./tfplan\n</code></pre> <pre><code>- step:\n    name: download_artifact\n    artifacts:\n        download: true\n</code></pre>"},{"location":"Git/comparing-github-actions-bitbucket-pipelines/#bitbucket-pipelines-secrets","title":"Bitbucket Pipelines Secrets","text":"<p>In Repository settings &gt; Pipelines &gt; Repository variables.</p> <p>Environment variables added on the repository level can be accessed by any users with push permissions in the repository. To access a variable, put the $ symbol in front of its name. For example, access AWS_ACCESS_KEY_ID by using $AWS_ACCESS_KEY_ID.</p>"},{"location":"Git/comparing-github-actions-bitbucket-pipelines/#bitbucket-pipelines-example","title":"Bitbucket Pipelines Example","text":"<p>The below is a Bitbucket Pipelines example which on push to master:</p> <ul> <li>Pulls a specific docker image of Terraform <code>1.5.5</code></li> <li>Performs a terraform initialisation using stored AWS credentials</li> <li>Performs a terraform plan</li> <li>Uploads the terraform plan output to artifacts</li> <li>Downloads the terraform plan from artifacts for a second step</li> <li>Performs a terraform init and then a terraform apply from the previous job using artifacts</li> </ul> <p></p> <pre><code>image: hashicorp/terraform:1.5.5\npipelines:\n    branches:\n        master:\n            - step:\n                name: 000base_plan\n                script:\n                    - cd layers/000base\n                    - terraform init -backend-config=\"access_key=$AWS_ACCESS_KEY_ID\" -backend-config=\"secret_key=$AWS_SECRET_ACCESS_KEY\"\n                    - terraform validate\n                    - terraform plan -no-color -out 000base_tfplan\n                artifacts:\n                  - layers/000base/000base_tfplan\n            - step:\n                name: 000base_apply\n                trigger: manual\n                artifacts:\n                  download: true\n                script:\n                    - cd layers/000base\n                    - terraform init -backend-config=\"access_key=$AWS_ACCESS_KEY_ID\" -backend-config=\"secret_key=$AWS_SECRET_ACCESS_KEY\"\n                    - terraform validate\n                    - terraform apply -auto-approve 000base_tfplan\n</code></pre>"},{"location":"Git/setting-up-github-pages/","title":"Setting up Github Pages","text":""},{"location":"Git/setting-up-github-pages/#pre-requisites","title":"Pre-requisites","text":""},{"location":"Git/setting-up-github-pages/#install-mkdocs","title":"Install mkdocs","text":"<p>Follow the below link to install <code>MKDocs</code></p> <p>\"Getting Started with MKDocs\"</p> <p>If you get the error <code>ModuleNotFoundError: No module named 'apt_pkg'</code> run the below:</p> <pre><code>sudo apt remove python3-apt\nsudo apt autoremove\nsudo apt autoclean\nsudo apt install python3-apt\n</code></pre>"},{"location":"Git/setting-up-github-pages/#install-make","title":"Install Make","text":"<p>Run the below in your terminal</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\nsudo apt install -y make\n</code></pre>"},{"location":"Git/setting-up-github-pages/#make-lint-permissions","title":"Make lint permissions","text":"<p>If you get <code>permissions denied</code> on the linting bash script, run the below:</p> <pre><code>sudo chmod a+x markdown-lint.sh\n</code></pre>"},{"location":"Git/setting-up-github-pages/#install-poetry","title":"Install Poetry","text":"<pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>If you get the error <code>/bin/bash: line 1: poetry: command not found</code>, ensure you have the correct path in your bash profile.</p> <p>Run:</p> <pre><code>sudo nano ~/.bash_profile\n</code></pre> <p>Add the below:</p> <pre><code>export PATH=\"/home/curtis/.local/bin:$PATH\"\n</code></pre>"},{"location":"Git/setting-up-github-pages/#set-up-github-actions","title":"Set up Github actions","text":"<p>Run through the \"Quickstart for GitHub Pages\".</p> <p>Ensure you have created a branch called <code>gh-pages</code></p> <p>Go to your repo, actions and new workflow and add the following YAML template:</p> <p>\"Github Actions Workflow\"</p>"},{"location":"Git/sync-github-to-bitbucket/","title":"Syncing Github repository to Bitbucket","text":""},{"location":"Git/sync-github-to-bitbucket/#bitbucket","title":"Bitbucket","text":"<p>Log into your Bitbucket account, go to your chosen Repository.</p> <p>Under <code>Security</code> choose <code>Access Tokens</code> and <code>Create Repository Access Token</code>.</p> <p>Set a <code>Name</code> and allow the permissions:</p> <ul> <li>Repositories:Read</li> <li>Repositories:Write</li> </ul> <p>Copy your x-token-auth URL it should look like:</p> <p><code>git clone https://x-token-auth:abcdefghijklmnopqrstuvwxyz1234567890@bitbucket.org/workspace/repository.git</code></p>"},{"location":"Git/sync-github-to-bitbucket/#github","title":"Github","text":"<p>Log into your Github account, go to your chosen Repository.</p> <p>Under <code>Settings</code> choose <code>Settings</code>, under <code>Security</code> and <code>Secrets and Variables</code> choose <code>Actions</code>.</p> <p>Create a <code>New Repository Secret</code> named <code>AUTH_TOKEN</code>.</p>"},{"location":"Git/sync-github-to-bitbucket/#github-actions","title":"Github Actions","text":"<p>Then in your repository add the following to your workflow file located in <code>.github/workflows</code>.</p> <pre><code>bitbucket_mirror:\n    name: Mirror to Bitbucket\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout the repository to the runner\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 'true'\n      - name: Checkout repo\n        run: git push --mirror https://x-token-auth:${{ secrets.AUTH_TOKEN }}@bitbucket.org/workspace/repository.git\n</code></pre>"},{"location":"Kubernetes/set-up-k8s-locally/","title":"Setting up a Kubernetes cluster locally on your machine","text":"<p>This document presumes you are running a windows machine.</p>"},{"location":"Kubernetes/set-up-k8s-locally/#prerequisites","title":"Prerequisites","text":"<p>Follow the \"Install Docker Desktop on Windows\"</p> <p>Install Kind via chocolatey</p> <pre><code>choco install kind\n</code></pre>"},{"location":"Kubernetes/set-up-k8s-locally/#use-kind-to-create-your-k8-cluster","title":"Use Kind to create your K8 cluster","text":"<p>Run the below:</p> <pre><code>kind create cluster\n</code></pre> <p>To see your cluster information run:</p> <pre><code>kubectl cluster-info --context kind-kind\n</code></pre> <p></p>"},{"location":"Kubernetes/set-up-k8s-locally/#use-kind-to-create-a-k8-cluster-with-multiple-nodes","title":"Use Kind to create a K8 cluster with multiple nodes","text":"<p>Save the below to a file named <code>kind.config</code></p> <pre><code># this config file contains all config fields with comments\n# NOTE: this is not a particularly useful config file\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\n# patch the generated kubeadm config with some extra settings\nkubeadmConfigPatches:\n- |\n  apiVersion: kubelet.config.k8s.io/v1beta1\n  kind: KubeletConfiguration\n  evictionHard:\n    nodefs.available: \"0%\"\n# patch it further using a JSON 6902 patch\nkubeadmConfigPatchesJSON6902:\n- group: kubeadm.k8s.io\n  version: v1beta3\n  kind: ClusterConfiguration\n  patch: |\n    - op: add\n      path: /apiServer/certSANs/-\n      value: my-hostname\n# 1 control plane node and 3 workers\nnodes:\n# the control plane node config\n- role: control-plane\n# the three workers\n- role: worker\n- role: worker\n- role: worker\n</code></pre> <p>Run the below referencing your <code>kind.config</code></p> <pre><code>kind create cluster --config '.\\Documents\\Kind\\kind.config'\n</code></pre> <p>Now when you run get nodes you can see a control-plane and 3 worker nodes</p> <pre><code>kubectl get nodes --context kind-kind\n</code></pre> <p></p>"},{"location":"Minecraft/deploy-a-minecraft-server/","title":"How to deploy a Minecraft Server","text":"<p>Use the Docker image <code>itzg/minecraft-bedrock-server</code>.</p>"},{"location":"Minecraft/deploy-a-minecraft-server/#server-properties","title":"Server Properties","text":"<p>You can edit things like <code>tick distance</code>.</p> <pre><code>tick-distance=4\n# The world will be ticked this many chunks away from any player.\n# Allowed values: Integers in the range [4, 12]\n</code></pre> <p>\"Server Properties\"</p>"},{"location":"Minecraft/deploy-a-minecraft-server/#useful-commands","title":"Useful commands","text":"<p>You can run commands in the container terminal</p> <p>Whitelist users:</p> <p><code>allowlist add &lt;USERNAME&gt;</code></p> <p>Set the setting to show coordinates:</p> <p><code>gamerule showcoordinates true</code></p>"},{"location":"Mkdocs/bracket-render-error/","title":"How to resolve the Macro Rendering Error","text":"<p>When putting things in a <code>{}</code> bracket within a YAML codeblock you get the error <code>jinja2.exceptions.UndefinedError: 'xxxxxx' is undefined</code>.</p> <p>The easiest way to output a literal variable delimiter is by using a variable expression, for example:</p> <pre><code>run: git push --mirror https://x-token-auth:${{ secrets.AUTH_TOKEN }}@bitbucket.org/workspace/repository.git\n</code></pre> <p>\"Escaping in Jinja\"</p>"},{"location":"Synology/resolve-mfa-error/","title":"Resolving 2FA on Synology NAS","text":"<p>If you get the below error it is likely the time between either your NAS or 2FA device is out of sync.</p> <p></p> <p>You can confirm whether it is the NAS with a time sync error by updating the time sync on your 2FA device, for example on Google authenticator you can go to:</p> <p><code>Settings &gt; Time correction for codes &gt; Sync now</code>.</p> <p>To sync the time for your NAS you will need to SSH onto the box and run the below as root:</p> <p><code>ntpdate -u time.google.com</code></p>"},{"location":"bitbucket_access_key_rotation/document/","title":"Document on the Bitbucket pipeline access key rotation","text":""},{"location":"bitbucket_access_key_rotation/document/#overview","title":"Overview","text":""},{"location":"bitbucket_access_key_rotation/document/#aws-access-key-rotation-for-bitbucket-pipeline-user","title":"AWS Access Key Rotation for Bitbucket Pipeline User","text":""},{"location":"bitbucket_access_key_rotation/document/#overview_1","title":"Overview","text":"<p>We have a terraform pipeline role <code>terraform-pipeline-role</code> in each AWS account deployed via Cloudformation stacksets, this is used in the Bitbucket pipeline to run the terraform plan/apply.</p> <p>There is a terraform pipeline user <code>terraform-pipeline-user</code> in the shared services account that has permissions to assume the terraform role in each AWS account.</p> <p>The AWS Access and Secret key for the terraform user is stored in each repository as a variable, with the AWS Secret key being a secret variable.</p> <p>This Python project uses Boto3 to firstly create the AWS Access keys for <code>terraform-pipeline-user</code>, store the keys in each repository variable and store them locally in Secrets manager as well as the variable UUID which is needed to update existing bitbucket variables.</p> <p>It then rotates AWS access keys for <code>terraform-pipeline-user</code>, updates the Bitbucket variables as well as the local AWS secrets</p>"},{"location":"bitbucket_access_key_rotation/document/#requirements","title":"Requirements","text":"<ul> <li><code>Python</code> 3.8+</li> <li><code>Boto3</code></li> <li><code>Requests</code> (for Bitbucket API calls)</li> <li>Bitbucket repository access tokens with:<ul> <li><code>pipeline:variable</code> permission</li> </ul> </li> <li>AWS Account to store Secrets and run the Lambda function</li> <li>AWS Secret named <code>bitbucket_access_tokens</code> with:<ul> <li>Key Value Access tokens for each repository in the format <code>access_token_{repository_name}:Value</code></li> </ul> </li> <li>AWS Secret named <code>bitbucket_repo_variable_uuids</code> with:<ul> <li>Key Value Placeholder secrets for each repository in the format:<ul> <li><code>AWS_ACCESS_KEY_ID_uuid_{repository_name}:Placeholder</code></li> <li><code>AWS_SECRET_ACCESS_KEY_uuid_{repository_name}:Placeholder</code></li> </ul> </li> </ul> </li> <li>AWS SNS Topic to send success notifications to</li> <li>AWS IAM permissions:<ul> <li><code>iam:CreateAccessKey</code></li> <li><code>iam:DeleteAccessKey</code></li> <li><code>iam:ListAccessKeys</code></li> <li><code>secretsmanager:ListSecrets</code></li> <li><code>secretsmanager:GetSecretValue</code></li> <li><code>secretsmanager:CreateSecret</code></li> <li><code>secretsmanager:UpdateSecret</code></li> <li><code>secretsmanager:PutSecretValue</code></li> </ul> </li> <li>Variables needed:<ul> <li><code>BITBUCKET_WORKSPACE</code> (str) - The name of your Bitbucket Workspace</li> <li><code>REPOSITORY_LIST</code> (list) - The list of Bitbucket repositories</li> <li><code>REGION</code> (str) - The AWS Region</li> <li><code>SNS_TOPIC</code> (str) - The AWS SNS Topic to send notifications to</li> </ul> </li> </ul>"},{"location":"bitbucket_access_key_rotation/document/#usage","title":"Usage","text":"<p>The recommendation is to first run <code>create_initial_access_key.py</code> locally, this creates the initial Access/Secret Keys as well as the AWS Secret to store the keys.</p> <p><code>create_initial_access_key(repository_list, region, terraform_user_secret_name)</code></p> <p>To run the script <code>rotate_bitbucket_access_key.py</code> on a schedule it is recommended to create a Lambda function with an eventbridge trigger with the frequency of your choice.</p> <p>We'd need to update the script to <code>import os</code> and to pull the below variables from the Lambda environment variables:</p> <ul> <li>os.environ['BITBUCKET_WORKSPACE']</li> <li>os.environ['REPOSITORY_LIST']</li> <li>os.environ['REGION']</li> </ul>"},{"location":"bitbucket_access_key_rotation/document/#script-steps","title":"Script steps","text":""},{"location":"bitbucket_access_key_rotation/document/#create_initial_access_key","title":"create_initial_access_key","text":"<ul> <li>Create AWS Access Key for <code>pipeline_iam_username</code></li> <li>Create AWS Secret <code>terraform_user_secret_name</code> and add <code>AWS_ACCESS_KEY_ID</code></li> <li>Update AWS Secret <code>terraform_user_secret_name</code> and add <code>AWS_SECRET_ACCESS_KEY</code></li> <li>Get bitbucket repository access token from <code>BITBUCKET_ACCESS_TOKEN_PATH</code></li> <li>Pull <code>AWS_ACCESS_KEY_ID</code> from <code>TERRAFORM_USER_SECRET_PATH</code><ul> <li>Add <code>AWS_ACCESS_KEY_ID</code> to repo variable</li> <li>Get <code>UUID</code> of repo variable</li> <li>Add <code>UUID</code> to <code>BITBUCKET_VARIABLE_UUIDS_SECRET_PATH</code> as <code>AWS_ACCESS_KEY_ID_uuid_{repository_name}</code></li> </ul> </li> <li>Pull <code>AWS_SECRET_ACCESS_KEY</code> from <code>TERRAFORM_USER_SECRET_PATH</code><ul> <li>Add <code>AWS_SECRET_ACCESS_KEY</code> to repo variable</li> <li>Get <code>UUID</code> of repo variable</li> <li>Add <code>UUID</code> to <code>BITBUCKET_VARIABLE_UUIDS_SECRET_PATH</code> as <code>AWS_SECRET_ACCESS_KEY_uuid_{repository_name}</code></li> </ul> </li> </ul>"},{"location":"bitbucket_access_key_rotation/document/#rotate_bitbucket_access_key","title":"rotate_bitbucket_access_key","text":"<ul> <li>Get <code>AWS_ACCESS_KEY_ID</code> for <code>PIPELINE_USER</code> from Secret <code>TERRAFORM_USER_SECRET_PATH</code></li> <li>Delete existing IAM Access Key for <code>PIPELINE_USER</code></li> <li>Create new Access keys for <code>PIPELINE_USER</code></li> <li>Store the AWS Access key and Secret key in Secrets Manager</li> <li>For each repo:<ul> <li>Pull the bitbucket access token from Secrets Manager secret <code>BITBUCKET_ACCESS_TOKEN_PATH</code></li> <li>Get existing variable <code>AWS_ACCESS_KEY_ID</code> <code>UUID</code> from Secret <code>BITBUCKET_VARIABLE_UUIDS_SECRET_PATH</code></li> <li>Pull secret <code>AWS_ACCESS_KEY_ID</code> from Secret <code>TERRAFORM_USER_SECRET_PATH</code></li> <li>Update the repo variable <code>AWS_ACCESS_KEY_ID</code></li> <li>Get existing variable <code>AWS_SECRET_ACCESS_KEY</code> <code>UUID</code> from Secret <code>BITBUCKET_VARIABLE_UUIDS_SECRET_PATH</code></li> <li>Pull secret <code>AWS_SECRET_ACCESS_KEY</code> from <code>TERRAFORM_USER_SECRET_PATH</code></li> <li>Update the repo variable <code>AWS_SECRET_ACCESS_KEY</code></li> </ul> </li> <li>Send an SNS notification upon success</li> </ul>"},{"location":"bitbucket_access_key_rotation/document/#code","title":"Code","text":""},{"location":"bitbucket_access_key_rotation/document/#create_initial_access_key_1","title":"create_initial_access_key","text":"<pre><code>\"\"\"Create the initial AWS Access Key and store in Bitbucket pipeline variables.\"\"\"\n\nimport json\nimport logging\n\nimport boto3\nimport requests\n\nlogger = logging.getLogger()\nlogger.setLevel(\"INFO\")\n\nPIPELINE_USER = \"terraform-pipeline-user\"\nBITBUCKET_ACCESS_TOKEN_PATH = \"bitbucket_access_tokens\"\nTERRAFORM_USER_SECRET_PATH = \"terraform-pipeline-user_access_keys\"\nBITBUCKET_VARIABLE_UUIDS_SECRET_PATH = \"bitbucket_repo_variable_uuids\"\nBITBUCKET_WORKSPACE = \"MY_BITBUCKET_WORKSPACE\"\nREPOSITORY_LIST = [\"MY_REPOSITORY_1\", \"MY_REPOSITORY_2\"]\nREGION = \"eu-west-1\"\n\n\ndef create_aws_access_key(pipeline_iam_username, region, terraform_user_secret_name):\n    \"\"\"\n    Create AWS Access keys for the pipeline user and store in secrets manager.\n    Parameters:\n      pipeline_iam_username (str): The IAM user used in the terraform pipeline\n      region (str): The region for the secrets\n      terraform_user_secret_name (str): The Secret name for the terraform user access keys\n    \"\"\"\n    iam_client = boto3.client(\"iam\")\n    secrets_client = boto3.client(\"secretsmanager\", region_name=region)\n    iam_response = iam_client.create_access_key(UserName=pipeline_iam_username)\n    access_key_kv = {\"AWS_ACCESS_KEY_ID\": iam_response[\"AccessKey\"][\"AccessKeyId\"]}\n    secret_key_kv = {\n        \"AWS_SECRET_ACCESS_KEY\": iam_response[\"AccessKey\"][\"SecretAccessKey\"]\n    }\n\n    secrets_client.create_secret(\n        Name=terraform_user_secret_name, SecretString=json.dumps(access_key_kv)\n    )\n    secrets_response = secrets_client.list_secrets(\n        Filters=[\n            {\"Key\": \"name\", \"Values\": [terraform_user_secret_name]},\n        ],\n    )\n    secrets_client.update_secret(\n        SecretId=secrets_response[\"SecretList\"][0][\"ARN\"],\n        SecretString=json.dumps(secret_key_kv),\n    )\n\n\ndef get_bitbucket_access_token(repository_name, region, bitbucket_access_token_secret):\n    \"\"\"\n    Get the Bitbucket Access token for the repository.\n    Parameters:\n        repository_name (str): The repository name.\n        region (str): The region for the secrets\n        bitbucket_access_token_secret (str): The secret which stores the Bitbucket access tokens\n    \"\"\"\n    secrets_client = boto3.client(\"secretsmanager\", region_name=region)\n\n    response = secrets_client.get_secret_value(SecretId=bitbucket_access_token_secret)\n    secret_key = f\"access_token_{repository_name}\"\n    repo_access_token = eval(response[\"SecretString\"])[secret_key]\n    logging.info(\"Fetched the Bitbucket access token for %s\", repository_name)\n    return repo_access_token\n\n\ndef create_bitbucket_variable(\n    bitbucket_workspace,\n    repository_name,\n    bitbucket_access_token,\n    variable_key,\n    variable_secured,\n    region,\n    terraform_user_secret_name,\n    bitbucket_variable_uuids_secret_name,\n):\n    \"\"\"\n    Create the bitbucket variables.\n    Parameters:\n        bitbucket_workspace (str): The Bitbucket workspace name\n        repository_name (str): The repository name.\n        bitbucket_access_token (str): The Bitbucket access token\n        variable_key (str): The variable to create. Allowed values:\n            'AWS_ACCESS_KEY_ID'\n            'AWS_SECRET_ACCESS_KEY'\n        variable_secured (bool): Whether the variable is secured or not\n        region (str): The region for the secrets\n        terraform_user_secret_name (str): The Secret name for the terraform user access keys\n        bitbucket_variable_uuids_secret_name (str): The Secret name for the bitbucket variable UUID\n    \"\"\"\n    secrets_client = boto3.client(\"secretsmanager\", region_name=region)\n    secrets_response = secrets_client.list_secrets(\n        Filters=[\n            {\"Key\": \"name\", \"Values\": [terraform_user_secret_name]},\n        ],\n    )\n    secret_response = secrets_client.get_secret_value(\n        SecretId=secrets_response[\"SecretList\"][0][\"ARN\"],\n    )\n\n    secret_value = eval(secret_response[\"SecretString\"])[variable_key]\n    logging.info(\"Fetched the %s\", variable_key)\n\n    url = f\"https://api.bitbucket.org/2.0/repositories/{bitbucket_workspace}/{repository_name}/pipelines_config/variables\"  # noqa: E501\n\n    headers = {\n        \"Accept\": \"application/json\",\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {bitbucket_access_token}\",\n    }\n\n    payload = json.dumps(\n        {\n            \"type\": \"string\",\n            \"key\": f\"{variable_key}\",\n            \"value\": f\"{secret_value}\",\n            \"secured\": variable_secured,\n        }\n    )\n\n    response = requests.request(\"POST\", url, data=payload, headers=headers, timeout=20)\n    logger.info(\n        json.dumps(\n            json.loads(response.text), sort_keys=True, indent=4, separators=(\",\", \": \")\n        )\n    )\n    response_json = json.dumps(\n        json.loads(response.text), sort_keys=True, indent=4, separators=(\",\", \": \")\n    )\n    variable_uuid = json.loads(response_json)[\"uuid\"][1:-1]\n\n    secrets_response = secrets_client.list_secrets(\n        Filters=[\n            {\"Key\": \"name\", \"Values\": [bitbucket_variable_uuids_secret_name]},\n        ],\n    )\n    secret_key_kv = {f\"{variable_key}_uuid_{repository_name}\": variable_uuid}\n    secrets_client.update_secret(\n        SecretId=secrets_response[\"SecretList\"][0][\"ARN\"],\n        SecretString=json.dumps(secret_key_kv),\n    )\n\n\ndef create_initial_access_key(repository_list, region, terraform_user_secret_name):\n    \"\"\"\n    Rotate keys then update Secrets and Bitbucket variables.\n    Parameters:\n        repository_list (list): The list of Repositories to update\n        region (str): The region for the secrets\n        terraform_user_secret_name (str): The secret name for the terraform user keys\n    \"\"\"\n    create_aws_access_key(PIPELINE_USER, region, terraform_user_secret_name)\n\n    for repository in repository_list:\n        repo_access_token = get_bitbucket_access_token(\n            repository, region, BITBUCKET_ACCESS_TOKEN_PATH\n        )\n        create_bitbucket_variable(\n            BITBUCKET_WORKSPACE,\n            repository,\n            repo_access_token,\n            \"AWS_ACCESS_KEY_ID\",\n            False,\n            region,\n            terraform_user_secret_name,\n            BITBUCKET_VARIABLE_UUIDS_SECRET_PATH,\n        )\n        create_bitbucket_variable(\n            BITBUCKET_WORKSPACE,\n            repository,\n            repo_access_token,\n            \"AWS_SECRET_ACCESS_KEY\",\n            True,\n            region,\n            terraform_user_secret_name,\n            BITBUCKET_VARIABLE_UUIDS_SECRET_PATH,\n        )\n</code></pre>"},{"location":"bitbucket_access_key_rotation/document/#rotate_bitbucket_access_key_1","title":"rotate_bitbucket_access_key","text":"<pre><code>\"\"\"Rotate Terraform user aws access keys and update repo variables.\"\"\"\n\nimport json\nimport logging\n\nimport boto3\nimport requests\nfrom botocore.exceptions import ClientError\n\nlogger = logging.getLogger()\nlogger.setLevel(\"INFO\")\n\nPIPELINE_USER = \"terraform-pipeline-user\"\nBITBUCKET_ACCESS_TOKEN_PATH = \"bitbucket_access_tokens\"\nTERRAFORM_USER_SECRET_PATH = \"terraform-pipeline-user_access_keys\"\nBITBUCKET_VARIABLE_UUIDS_SECRET_PATH = \"bitbucket_repo_variable_uuids\"\nBITBUCKET_WORKSPACE = \"MY_BITBUCKET_WORKSPACE\"\nREPOSITORY_LIST = [\"MY_REPOSITORY_1\", \"MY_REPOSITORY_2\"]\nREGION = \"eu-west-1\"\nSNS_TOPIC = \"MY_SNS_TOPIC\"\n\n\ndef get_existing_access_key(region, terraform_user_secret_name):\n    \"\"\"\n    Retrieve the existing AWS Access Key.\n    Parameters:\n      region (str): The region for the secrets\n      terraform_user_secret_name (str): The Secret name for the terraform user access keys\n    \"\"\"\n    secrets_client = boto3.client(\"secretsmanager\", region_name=region)\n\n    response = secrets_client.get_secret_value(SecretId=terraform_user_secret_name)\n    existing_access_key = eval(response[\"SecretString\"])[\"AWS_ACCESS_KEY_ID\"]\n    logging.info(\"Existing AWS Access Key is %s\", existing_access_key)\n    return existing_access_key\n\n\ndef delete_existing_access_key(pipeline_iam_username, aws_access_key):\n    \"\"\"\n    Delete the existing AWS Access Key.\n    Parameters:\n      pipeline_iam_username (str): The IAM user used in the terraform pipeline\n      aws_access_key (str): The existing AWS Access Key\n    \"\"\"\n    iam_client = boto3.client(\"iam\")\n    try:\n        # Deleting the specified access key\n        response = iam_client.delete_access_key(\n            UserName=pipeline_iam_username, AccessKeyId=aws_access_key\n        )\n        logging.info(\n            \"Access key %s for user %s has been deleted successfully.\",\n            aws_access_key,\n            pipeline_iam_username,\n        )\n        return response\n    except ClientError as e:\n        # Handling errors such as non-existent user or invalid access key\n        logging.info(\n            \"Error deleting access key %s for user %s: %s\",\n            aws_access_key,\n            pipeline_iam_username,\n            e,\n        )\n        return None\n\n\ndef create_aws_access_key(pipeline_iam_username, region, terraform_user_secret_name):\n    \"\"\"\n    Create new AWS Access keys for the pipeline user and store in secrets manager.\n    Parameters:\n      pipeline_iam_username (str): The IAM user used in the terraform pipeline\n      region (str): The region for the secrets\n      terraform_user_secret_name (str): The Secret name for the terraform user access keys\n    \"\"\"\n    iam_client = boto3.client(\"iam\")\n    secrets_client = boto3.client(\"secretsmanager\", region_name=region)\n    iam_response = iam_client.create_access_key(UserName=pipeline_iam_username)\n    access_key_kv = {\"AWS_ACCESS_KEY_ID\": iam_response[\"AccessKey\"][\"AccessKeyId\"]}\n    secret_key_kv = {\n        \"AWS_SECRET_ACCESS_KEY\": iam_response[\"AccessKey\"][\"SecretAccessKey\"]\n    }\n\n    secrets_response = secrets_client.list_secrets(\n        Filters=[\n            {\"Key\": \"name\", \"Values\": [terraform_user_secret_name]},\n        ],\n    )\n    secrets_client.update_secret(\n        SecretId=secrets_response[\"SecretList\"][0][\"ARN\"],\n        SecretString=json.dumps(access_key_kv),\n    )\n    secrets_client.update_secret(\n        SecretId=secrets_response[\"SecretList\"][0][\"ARN\"],\n        SecretString=json.dumps(secret_key_kv),\n    )\n\n\ndef get_bitbucket_access_token(repository_name, region, bitbucket_access_token_secret):\n    \"\"\"\n    Get the Bitbucket Access token for the repository.\n    Parameters:\n        repository_name (str): The repository name.\n        region (str): The region for the secrets\n        bitbucket_access_token_secret (str): The secret which stores the Bitbucket access tokens\n    \"\"\"\n    secrets_client = boto3.client(\"secretsmanager\", region_name=region)\n\n    response = secrets_client.get_secret_value(SecretId=bitbucket_access_token_secret)\n    secret_key = f\"access_token_{repository_name}\"\n    repo_access_token = eval(response[\"SecretString\"])[secret_key]\n    logging.info(\"Fetched the Bitbucket access token for %s\", repository_name)\n    return repo_access_token\n\n\ndef get_variable_uuid(\n    region, bitbucket_variable_uuids_secret_name, variable_key, repository\n):\n    \"\"\"\n    Get the existing 'AWS_ACCESS_KEY_ID' or 'AWS_SECRET_ACCESS_KEY' repository variable UUID.\n    Parameters:\n        region (str): The region for the secrets\n        bitbucket_variable_uuids_secret_name (str): The Secret for the bitbucket variable UUIDs\n        variable_key (str): The variable to pull. Allowed values:\n            'AWS_ACCESS_KEY_ID'\n            'AWS_SECRET_ACCESS_KEY'\n        repository (str): The repository name.\n    \"\"\"\n    secrets_client = boto3.client(\"secretsmanager\", region_name=region)\n    response = secrets_client.get_secret_value(\n        SecretId=bitbucket_variable_uuids_secret_name\n    )\n    secret_key = f\"{variable_key}_uuid_{repository}\"\n    repo_variable_uuid = eval(response[\"SecretString\"])[secret_key]\n    logging.info(\n        \"Repository variable UUID for key %s and repository %s is %s\",\n        variable_key,\n        repository,\n        repo_variable_uuid,\n    )\n    return repo_variable_uuid\n\n\ndef create_bitbucket_variable(\n    bitbucket_workspace,\n    repository_name,\n    bitbucket_access_token,\n    variable_key,\n    variable_uuid,\n    variable_secured,\n    region,\n    terraform_user_secret_name,\n    bitbucket_variable_uuids_secret_name,\n):\n    \"\"\"\n    Create the bitbucket variables.\n    Parameters:\n        bitbucket_workspace (str): The Bitbucket workspace name\n        repository_name (str): The repository name.\n        bitbucket_access_token (str): The Bitbucket access token\n        variable_key (str): The variable to create. Allowed values:\n            'AWS_ACCESS_KEY_ID'\n            'AWS_SECRET_ACCESS_KEY'\n        variable_uuid (str): The UUID of the variable to update\n        variable_secured (bool): Whether the variable is secured or not\n        region (str): The region for the secrets\n        terraform_user_secret_name (str): The Secret name for the terraform user access keys\n        bitbucket_variable_uuids_secret_name (str): The Secret for the bitbucket variable UUIDs\n    \"\"\"\n    secrets_client = boto3.client(\"secretsmanager\", region_name=region)\n    secrets_response = secrets_client.list_secrets(\n        Filters=[\n            {\"Key\": \"name\", \"Values\": [terraform_user_secret_name]},\n        ],\n    )\n    secret_response = secrets_client.get_secret_value(\n        SecretId=secrets_response[\"SecretList\"][0][\"ARN\"],\n    )\n\n    secret_value = eval(secret_response[\"SecretString\"])[variable_key]\n    logging.info(\"Fetched the %s\", variable_key)\n\n    url = f\"https://api.bitbucket.org/2.0/repositories/{bitbucket_workspace}/{repository_name}/pipelines_config/variables\"  # noqa: E501\n\n    headers = {\n        \"Accept\": \"application/json\",\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {bitbucket_access_token}\",\n    }\n\n    payload = json.dumps(\n        {\n            \"type\": \"string\",\n            \"uuid\": f\"{variable_uuid}\",\n            \"key\": f\"{variable_key}\",\n            \"value\": f\"{secret_value}\",\n            \"secured\": variable_secured,\n        }\n    )\n\n    response = requests.request(\"POST\", url, data=payload, headers=headers, timeout=20)\n    logger.info(\n        json.dumps(\n            json.loads(response.text), sort_keys=True, indent=4, separators=(\",\", \": \")\n        )\n    )\n    response_json = json.dumps(\n        json.loads(response.text), sort_keys=True, indent=4, separators=(\",\", \": \")\n    )\n    variable_uuid = json.loads(response_json)[\"uuid\"][1:-1]\n\n    secrets_response = secrets_client.list_secrets(\n        Filters=[\n            {\"Key\": \"name\", \"Values\": [bitbucket_variable_uuids_secret_name]},\n        ],\n    )\n    secret_key_kv = {f\"{variable_key}_uuid_{repository_name}\": variable_uuid}\n    secrets_client.update_secret(\n        SecretId=secrets_response[\"SecretList\"][0][\"ARN\"],\n        SecretString=json.dumps(secret_key_kv),\n    )\n\n\ndef rotate_bitbucket_access_key(\n    region,\n    repository_list,\n    terraform_user_secret_name,\n    pipeline_iam_username,\n    sns_topic,\n):\n    \"\"\"\n    Rotate keys then update Secrets and Bitbucket variables.\n    Parameters:\n        repository_list (list): The list of Repositories to update\n        region (str): The region for the secrets\n        terraform_user_secret_name (str): The secret name for the terraform user keys\n        pipeline_iam_username (str): The IAM user used in the terraform pipeline\n        sns_topic (str): The SNS Topic to send notifications to\n    \"\"\"\n    sns_client = boto3.client(\"sns\", region_name=region)\n    existing_access_key = get_existing_access_key(region, terraform_user_secret_name)\n    delete_existing_access_key(pipeline_iam_username, existing_access_key)\n    create_aws_access_key(pipeline_iam_username, region, terraform_user_secret_name)\n    for repository in repository_list:\n        repo_access_token = get_bitbucket_access_token(\n            repository, region, BITBUCKET_ACCESS_TOKEN_PATH\n        )\n        access_key_uuid = get_variable_uuid(\n            region,\n            BITBUCKET_VARIABLE_UUIDS_SECRET_PATH,\n            \"AWS_ACCESS_KEY_ID\",\n            repository,\n        )\n        create_bitbucket_variable(\n            BITBUCKET_WORKSPACE,\n            repository,\n            repo_access_token,\n            \"AWS_ACCESS_KEY_ID\",\n            access_key_uuid,\n            False,\n            region,\n            terraform_user_secret_name,\n            BITBUCKET_VARIABLE_UUIDS_SECRET_PATH,\n        )\n        secret_key_uuid = get_variable_uuid(\n            region,\n            BITBUCKET_VARIABLE_UUIDS_SECRET_PATH,\n            \"AWS_SECRET_ACCESS_KEY\",\n            repository,\n        )\n        create_bitbucket_variable(\n            BITBUCKET_WORKSPACE,\n            repository,\n            repo_access_token,\n            \"AWS_SECRET_ACCESS_KEY\",\n            secret_key_uuid,\n            True,\n            region,\n            terraform_user_secret_name,\n            BITBUCKET_VARIABLE_UUIDS_SECRET_PATH,\n        )\n\n    sns_client.publish(\n        TopicArn=sns_topic,\n        Subject=f\"{pipeline_iam_username} - Access Key Rotation\",\n        Message=(\n            \"\".join(\n                [\n                    f\"AWS Access key rotated for {pipeline_iam_username}.\"\n                    \"The following Bitbucket repositories have had the variables updated\",\n                    \"\\n\",\n                    \"\\n\",\n                    f\"{repository_list}\",\n                ]\n            )\n        ),\n    )\n</code></pre>"},{"location":"studying/certified_kubernetes_application_developer_ckad/application-design-and-build/","title":"Application design and build","text":""},{"location":"studying/certified_kubernetes_application_developer_ckad/application-design-and-build/#application-design-and-build","title":"Application Design and Build","text":""},{"location":"studying/certified_kubernetes_application_developer_ckad/application-design-and-build/#overview","title":"Overview","text":"<p>Topics:</p> <ul> <li>Images</li> <li>Jobs and CronJobs</li> <li>Multi-Container Pods</li> <li>Init containers</li> <li>Container Storage</li> </ul>"},{"location":"studying/certified_kubernetes_application_developer_ckad/application-design-and-build/#building-container-images","title":"Building Container Images","text":"<ul> <li>What is a container image</li> <li>Role of Docker and Dockerfile</li> </ul>"},{"location":"studying/certified_kubernetes_application_developer_ckad/application-design-and-build/#what-is-a-container-image","title":"What is a Container Image","text":"<p>An Image is a lightweight, standalone file that contains the software and executable needed to run a container.</p> <p>A Dockerfile defines what is container in the image.</p> <p>The <code>docker build</code> command builds an image using the Dockerfile.</p>"},{"location":"studying/certified_kubernetes_application_developer_ckad/application-design-and-build/#building-the-image","title":"Building the image","text":"<p>Run:</p> <pre><code>docker build -t my-website:0.0.1 .\n</code></pre> <p>The <code>-t</code> flag allows you to tag the image.</p> <p>The <code>.</code> flag looks within the current directory.</p> <p>Then run:</p> <pre><code>docker run --rm --name my-website -d -p 8080:80 my-website:0.0.1\n</code></pre> <p>The <code>-rm</code> flag deletes the container once it is stopped.</p> <p>The <code>-name</code> flag gives it a name.</p> <p>The <code>-d</code> flag runs it in detached mode.</p> <p>The <code>-p 8080:80</code> flag exposes port 80 to port 8080.</p> <p>You can test the container by running <code>curl localhost:8080</code>.</p> <p>Then run <code>docker stop my-website</code> to stop the container.</p>"},{"location":"studying/certified_kubernetes_application_developer_ckad/application-design-and-build/#saving-the-image","title":"Saving the image","text":"<p>Run:</p> <pre><code>docker save -o ./my-website:0.0.1.tar my-website:0.0.1\n</code></pre> <p>The <code>-o</code> flag sets the location.</p>"},{"location":"studying/certified_kubernetes_application_developer_ckad/application-design-and-build/#running-jobs-and-cronjobs","title":"Running Jobs and Cronjobs","text":"<ul> <li>What is a Job?</li> <li>What is a Cronjob?</li> </ul>"},{"location":"studying/certified_kubernetes_application_developer_ckad/application-design-and-build/#what-is-a-job","title":"What is a Job","text":"<p>Jobs are designed to run a containerised task successfully to completion.</p>"},{"location":"studying/certified_kubernetes_application_developer_ckad/application-design-and-build/#what-is-a-cronjob","title":"What is a CronJob","text":"<p>Cronjobs run Jobs periodically according to a schedule.</p>"},{"location":"studying/certified_kubernetes_application_developer_ckad/application-design-and-build/#example-job","title":"Example job","text":"<p>This container below will run the code and stop.</p> <p>The option <code>restartPolicy: Never</code> means it will never restart.</p> <p>The option <code>backoffLimit: 4</code> means it will try a retry 4 times if it fails.</p> <p>The option <code>activeDeadlineSeconds: 10</code> means kubernetes will only let it run for 10 seconds.</p> <p>```YAML apiVersion: batch/v1 kind: Job metadata:   name: my-job spec:   template:     spec:       containers:       - name: print       image: busybox:stable       command: [\"echo\", \"This is a test!\"]     restartPolicy: Never   backoffLimit: 4   activeDeadlineSeconds: 10</p>"},{"location":"studying/certified_kubernetes_application_developer_ckad/general/","title":"General","text":""},{"location":"studying/certified_kubernetes_application_developer_ckad/general/#ckad-overview","title":"CKAD Overview","text":""},{"location":"studying/certified_kubernetes_application_developer_ckad/general/#introduction","title":"Introduction","text":"<p>The Certified Kubernetes Application Developer (CKAD) exam certifies that candidates can design, build and deploy cloud-native applications for Kubernetes.</p> <p>The exam is hands on. You'll need to perform tasks in k8 clusters.</p> <p>The exam time limit is 2 hours.</p> <p>The exam is open documentation, the below sites are allowed:</p> <ul> <li>https://kubernetes.io/docs</li> <li>https://kubernetes.io/blog</li> <li>https://github.com/kubernetes</li> <li>https://helm.sh/docs</li> </ul>"},{"location":"studying/certified_kubernetes_application_developer_ckad/general/#domains-competencies","title":"Domains &amp; Competencies","text":"<ul> <li>20% - Application Design and Build</li> <li>20% - Application Deployment</li> <li>15% - Application Observability and Maintenance</li> <li>25% - Application Environment, Configuration and Security</li> <li>20% - Services and Networking</li> </ul>"},{"location":"studying/solutions_architect_professional/","title":"Index","text":""},{"location":"studying/solutions_architect_professional/#sa-pro-overview","title":"SA Pro Overview","text":"<p>180 minutes to answer 75 multiple choice questions.</p> <p>750/1000 generally needed to pass.</p>"},{"location":"studying/solutions_architect_professional/#introduction","title":"Introduction","text":"<p>Exam secions:</p> <ul> <li>Datastores</li> <li>Networking</li> <li>Security</li> <li>Migrations</li> <li>Architecting to scale</li> <li>Business continuity</li> <li>Deployment and operations management</li> <li>Cost management</li> </ul>"},{"location":"studying/solutions_architect_professional/#exam-guide","title":"Exam guide","text":"<p>Four domains:</p> <ul> <li>Design solutions for organisational complexity - 26%</li> <li>Design for new solutions - 29%</li> <li>Continuous improvement for existing solutions - 25%</li> <li>Accelerate workload migration and modernisation - 20%</li> </ul>"},{"location":"studying/solutions_architect_professional/#acg-course","title":"ACG course","text":"<ul> <li>11 hours 30 mins - Lessons</li> <li>17 hours 45 mins - Labs</li> <li>3 hours 45 mins - Quiz</li> <li>9 hours - Practice exam</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/","title":"Architecting to scale","text":""},{"location":"studying/solutions_architect_professional/architecting_to_scale/#architecting-to-scale","title":"Architecting to Scale","text":""},{"location":"studying/solutions_architect_professional/architecting_to_scale/#design-solutions-for-organisational-complexity","title":"Design Solutions for Organisational Complexity","text":"<ul> <li>AWS right sizing tools - AWS Compute Optimizer and AWS S3 Storage Lens</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#design-for-new-solutions","title":"Design for new solutions","text":"<ul> <li>Auto scaling policies</li> <li>Service quotas and limits</li> <li>Performance monitoring</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#continuous-improvement-for-existing-solutions","title":"Continuous Improvement for Existing Solutions","text":"<ul> <li>Monitoring and logging</li> <li>Instance fleets</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#accelerate-workload-migration-and-modernisation","title":"Accelerate Workload Migration and Modernisation","text":"<ul> <li>Containers</li> <li>Serverless</li> <li>Integration services - SQS, SNS, Step functions</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#architecting-to-scale-overview","title":"Architecting to Scale overview","text":"<p>Loosely coupled architecture - Components can work independently and require little to no knowledge of the inner workings of other components.</p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#autoscaling","title":"Autoscaling","text":""},{"location":"studying/solutions_architect_professional/architecting_to_scale/#ec2-autoscaling-cooldown","title":"EC2 Autoscaling Cooldown","text":"<ul> <li>Configurable duration that gives your scaling a chance to come up to speed</li> <li>Default is 300 seconds</li> <li>Automatically applies to dynamic scaling, it's optional to manual scaling but not supported for scheduled</li> <li>Can override cooldown</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#aws-autoscaling","title":"AWS Autoscaling","text":"<ul> <li>Target tracking scaling \u2013 Scale a resource based on a target value for a specific CloudWatch metric.</li> <li>Step scaling \u2013 Scale a resource based on a set of scaling adjustments that vary based on the size of the alarm breach.</li> <li>Scheduled scaling \u2013 Scale a resource one time only or on a recurring schedule.</li> </ul> <p>Example for DynamoDB scaling:</p> <p></p> <p>Predictive scaling uses machine learning to learn your load and calculate expected capacity</p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#compute-optimizer","title":"Compute Optimizer","text":"<ul> <li>Machine learning tool to give recommendations on compute resources</li> <li>Increase efficiency by reconfiguring over-provisioned resources</li> <li>Improve performance for under-provisioned resources</li> <li>Can be activated in one account or in an organisation</li> </ul> <p>Compatible resources:</p> <ul> <li>EC2 - instance types and optimising ASGs</li> <li>EBS - Recommendations on volume types and sizes</li> <li>ECS on Fargate - Optimise task size and container size</li> <li>Lambda - Optimise the CPU and Memory allocation</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#kinesis","title":"Kinesis","text":"<p>Collection of services for processing streams of various data</p> <p>Data is processed in <code>shards</code> which are able to ingest 1000 records per second each</p> <p>A default limit of 500 shards is set but you can request an increase to unlimited</p> <p>Data records ingested consist of:</p> <ul> <li>Partition key</li> <li>Sequence number</li> <li>Data blob (up to 1MB)</li> </ul> <p>Not persistent storage, default retention is 25 hours, can be configured to 365 days but should be used for transient storage moving data to another service to be ingested</p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#kinesis-data-streams","title":"Kinesis Data streams","text":"<p>Ingest high volume of data and process it in a number of ways</p> <p></p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#kinesis-firehouse","title":"Kinesis Firehouse","text":"<p>Prepares and loads the data to a destination of choice</p> <p></p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#kinesis-data-analytics","title":"Kinesis Data analytics","text":"<p>Can run analytics on the data as it's coming in, can run standard SQL queries against the data streams</p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#kinesis-shards","title":"Kinesis Shards","text":"<p>Like lanes in a motorway, the more lanes, the more traffic that can go through</p> <p>Then you can either use applications to process the data in the streams or firehose to send that data to another service</p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#kinesis-example","title":"Kinesis Example","text":"<ul> <li>Pull in tweets from Twitter API</li> <li>Send them to a Kinesis stream</li> <li>Kinesis firehose sends the data to an S3 bucket</li> <li>Lambda parses the data and stores it in DynamoDB</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#dynamodb-auto-scaling","title":"DynamoDB Auto scaling","text":"<p>Two ways to scale DynamoDB</p> <ul> <li>Throughput - based on RCU and WCUs</li> <li>Size - based on the storage size of data (max item is 400kb)</li> </ul> <p></p> <ul> <li>Partition - a physical space where DynamoDB data is stored</li> <li>Partition key - a Unique identifier for each record, sometimes known as a Hash key</li> <li>Sort key - In combination with the partition key, optional second part of a composite key that defines storage order</li> </ul> <p>Formula to decide on how many partitions are in a table:</p> Method Calculation Note By Capacity (Total RCU / 3000) + (Total WCU / 1000) How many read units and write units are provisioned By Size Total Size / 10GB Number of 10GB chunks the data takes up Total Partitions Round Up for the MAX (By Capacity, By Size) The max of either of the above <p>Example:</p> <ul> <li>2000 RCUs</li> <li>2000 WCUs</li> <li>10GB Data</li> </ul> Method Calculation Value By Capacity (2000 / 3000) + (2000 / 1000) 2.66 By Size Total Size / 10GB 1 Total Partitions Round Up for the MAX (By Capacity, By Size) 2.66 - So 3 partitions <p>If you use say the Date as the partition key, and there's a lot of read/writes for that particular day then you end up with a <code>Hot partition</code> in that all traffic is only hitting the one partition:</p> <p></p> <p>A good idea would be to  use the sensor_id as the partition key which distributes the data across multiple partitions, and then use the date as the sort key.</p> <p></p> <p>Can use autoscaling and scale on capacity.</p> <ul> <li>Cannot scale down if the consumption drops to zero</li> <li>Can send minimal traffic which causes autoscaling to realise the traffic is back to a low amount</li> <li>Can also manually reduce the max capacity to be the same as the minimum</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#dynamodb-on-demand-scaling","title":"DynamoDB On-Demand scaling","text":"<p>Useful if you can't deal with scaling lag or have no idea of the anticipated capacity requirements</p> <p>It instantly allocates capacity as needed with no concept of provisioned capacity</p> <p>This ends up costing more than traditional provisioning or autoscaling</p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#dynamodb-accelerator-dax","title":"DynamoDB Accelerator - DAX","text":"<p>This is an in memory cache that provides micro-second responses.</p> <p>Use cases:</p> <ul> <li>Requires fastest possible reads such as live auctions or financial trading</li> <li>Read-intense scenarios where you want to offload reads from the DynamoDB</li> <li>Repeated reads against a large set of DynamoDB data</li> </ul> <p>Generally wouldn't use DAX on a write heavy scenario, just read.</p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#cloudfront","title":"Cloudfront","text":"<p>Can deliver content to your users by caching static and dynamic content at AWS edge locations</p> <p>Dynamic content delivery is achieved using HTTP cookies forwarded from your origin</p> <p>Supports Adobe Flash Media Server RMTP protocol but you have do specifically choose RMTP delivery method.</p> <p>Web distributions also support media and live streaming but use HTTP/HTTPS</p> <p>Origins can be:</p> <ul> <li>S3</li> <li>EC2</li> <li>ELB</li> </ul> <p>Invalidate a cloudfront cache:</p> <ul> <li>Simply delete the file from the origin and wait for the TTL to expire</li> <li>Use the console to request an invalidation on all content or you can specify for example /images/*</li> <li>Use Cloudfront API to submit an invalidation</li> <li>Use third-party tool such as CFN Planet cloudfront purge tool</li> </ul> <p>Cloudfront supports geo-restrictions to blacklist regions</p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#event-driven-architecture","title":"Event driven architecture","text":"<p>A way to trigger asynchronous events across your AWS environments based on events</p> <p>An event is any change in state or an update signal sent by an event producer</p> <p>Event routers filter and push the events and their payloads to the relevant service</p> <p>Serverless:</p> <ul> <li>No patches or OS management</li> <li>Flexible scaling based on the load</li> <li>High availability</li> <li>Scales to zero when not in use</li> </ul> <p>Some services do not scale to zero:</p> <ul> <li>Aurora serverless</li> <li>Neptune serverless</li> <li>EMR serverless</li> <li>Opensearch serverless</li> </ul> <p>These are generally best for a 'serverless' environment but there are minimum payments for use</p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#sns","title":"SNS","text":"<p>A good use of SNS is to split out a workflow into different services/jobs from one trigger, for example:</p> <p></p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#sqs","title":"SQS","text":"<ul> <li>Scalable hosted message queueing service</li> <li>Integrates with KMS for encrypted messages</li> <li>Transient storage, not persistent - default is 4 days, max is 14 days</li> <li>Max message size of 256KB but using Java SQS SDK you can have messages as large as 2GB<ul> <li>Essentially this stores the message on S3 and the mssage points to this file</li> </ul> </li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#amazon-mq","title":"Amazon MQ","text":"<ul> <li>Functions the same as SQS but is an implementation of Apache ActiveMQ</li> <li>HG within a region</li> <li>ActiveMQ API</li> <li>Designed as a drop-in replacement for on-premis message brokers</li> <li>SQS is better for new environments</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#aws-serverless-application-model-sam","title":"AWS Serverless Application Model (SAM)","text":"<ul> <li>Open source framework for building serverless apps on AWS</li> <li>Uses YAML as the configuration language</li> <li>Includes a CLI-like functionality to create and update serverless apps</li> <li>Enables local testing and debugging of apps <code>sam local start-api</code></li> <li>Extension of Cloudformation</li> </ul> <p>AWS have a Serverless application repository, can download a pre-built application</p> <p>Not to be confused with Serverless framework, this is multi-cloud, not just AWS.</p>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#scaling-containers","title":"Scaling Containers","text":""},{"location":"studying/solutions_architect_professional/architecting_to_scale/#app-runner","title":"App runner","text":"<ul> <li>Designed exclusively for Synchronous HTTP applications</li> <li>Supplies compute and networking for container images, Python, Java or Node.js projects</li> <li>Supports public and private endpoints</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#batch","title":"Batch","text":"<ul> <li>Run thousands of containerised batch jobs</li> <li>Plans, schedules and executes your compute workloads</li> <li>Dynamically provisions CPU and memory optimised compute resources based on needs</li> <li>Runs the jobs as ECS, EKS or Fargate</li> <li>Can use spot instances</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#step-functions","title":"Step functions","text":"<ul> <li>Managed workflow</li> <li>Define your app as a state machine<ul> <li>Within the state machine you can create tasks, steps, timers</li> </ul> </li> <li>ASL declarative JSON</li> <li>Visual interface to see the workflow</li> <li>Apps can update the stream via Step Function API</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#elastic-map-reduce-emr","title":"Elastic Map Reduce (EMR)","text":"<ul> <li>Managed Hadoop framework for processing huge amounts of data</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#amazon-quicksight","title":"Amazon Quicksight","text":"<ul> <li>Serverless pay-per use service</li> <li>Low cost compare to other BI solutions</li> <li>SPICE and Autograph technologies</li> </ul> <p>Supported data sources:</p> <ul> <li>Athena</li> <li>Aurora</li> <li>Redshift</li> <li>S3</li> <li>RDS</li> <li>Opensearch</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#opensearch","title":"Opensearch","text":"<ul> <li>Open-source search and analytics suite forked from elasticsearch</li> <li>AWS provide Opensearch clusters</li> <li>Create dashboards or use built-in Kibana</li> <li>Visualise, search and analyse data in real time</li> <li>More advanced querying and lower cost at scale compared to cloudwatch</li> </ul>"},{"location":"studying/solutions_architect_professional/architecting_to_scale/#quiz","title":"Quiz","text":"<p>Your DynamoDB table has approximately 25 GB of data and 1000 RCUs. How many partitions can we expect for this table assuming the use of provisioned mode?</p> <ul> <li>Unable to determine</li> </ul> <p>Note</p> <p>We can't determine without the WCUs.</p> <p>When developing a Amazon Kinesis Data Stream application, what is the recommended method to read data from a shard?</p> <ul> <li>Amazon Kinesis Client Library (KCL)</li> </ul> <p>Note</p> <p>AWS recommends using Amazon Kinesis Client Library (KCL) if applicable because it performs heavy-lifting tasks associated with distributed stream processing, making it more productive to develop applications. You can only use Amazon Kinesis API to build your Amazon Kinesis Application</p> <p>You are designing a DynamoDB datastore to record electric meter readings from millions of homes once a week. We share on our website weekly live electric consumption charts based of this data so the week must be part of the primary key. How might we design our datastore for optimal efficiency?</p> <ul> <li>Use a table per week to store the data.</li> </ul> <p>Note</p> <p>General design principles in Amazon DynamoDB recommend that you keep the number of tables you use to a minimum. For most applications, a single table is all you need. However, for time series data, you can often best handle it by using one table per application per period. If we put all the time series data in one big table, the last partition is the one that gets all the read and write activity, limiting the throughput. If we create a new table for each period, we can maximize the RCU and WCU efficiency against a smaller number of partitions.</p> <p>In CloudFront, Behaviors permit which of the following scenarios?</p> <ul> <li>Delivery of different origins based on URL path</li> </ul> <p>Note</p> <p>Behaviors allow us to define different origins depending on the URL path. This is useful when we want to serve up static content from S3 and dynamic content from an EC2 fleet for example for the same website.</p>"},{"location":"studying/solutions_architect_professional/business_continuity/","title":"Business continuity","text":""},{"location":"studying/solutions_architect_professional/business_continuity/#business-continuity","title":"Business Continuity","text":""},{"location":"studying/solutions_architect_professional/business_continuity/#design-solutions-for-organisational-complexity","title":"Design Solutions for Organisational Complexity","text":"<ul> <li>RTO and RPO</li> <li>Disaster recover strategy</li> <li>Pilot light, warm stand-by, multi-site</li> </ul>"},{"location":"studying/solutions_architect_professional/business_continuity/#design-for-new-solutions","title":"Design for new solutions","text":"<ul> <li>DR solutions on AWS</li> </ul>"},{"location":"studying/solutions_architect_professional/business_continuity/#continuous-improvement-for-existing-solutions","title":"Continuous Improvement for Existing Solutions","text":"<ul> <li>Creating a DR plan</li> <li>Backup practices</li> <li>HA and resiliency</li> </ul>"},{"location":"studying/solutions_architect_professional/business_continuity/#accelerate-workload-migration-and-modernisation","title":"Accelerate Workload Migration and Modernisation","text":"<ul> <li>Networking in migration scenarios</li> </ul>"},{"location":"studying/solutions_architect_professional/business_continuity/#business-continuity-overview","title":"Business Continuity Overview","text":"<ul> <li>Backup and Restore - Back up the infrastructure and host it somewhere else such as Rackspace</li> <li>Pilot light - Have a minimal infrastructure hosted elsewhere, somewhere like Rackspace but need manual setup during a DR situation</li> <li>Warm standby - Have a minimal infrastructure hosted elsewhere, somewhere like Rackspace but most of the infra is setup and needs minimal configuration during a DR situation</li> <li>Multi-site - Have a second environment hosted elsewhere all configured ready for a failover during a DR situation using something like a DNS update</li> </ul>"},{"location":"studying/solutions_architect_professional/business_continuity/#ebs","title":"EBS","text":"<ul> <li>SLA target of 99.999%</li> <li>Replicates within the AZ</li> <li>However this means it's vulnerable to an AZ failure</li> <li>Can set up RAID on an EC2 instance with EBS</li> </ul> <p>RAID5 and RAID6 is not advised for EBS on EC2 as traffic goes over the network and the parity writes can cause a lot of i/o</p> <p></p>"},{"location":"studying/solutions_architect_professional/business_continuity/#s3","title":"S3","text":"<ul> <li>Standard class - 99.99% availability</li> <li>Standard infrequent access - 99.9% availability</li> <li>One-zone infrequent access - 99.5% availability</li> </ul> <p>Standard and Standard IA have multi-az durability</p>"},{"location":"studying/solutions_architect_professional/business_continuity/#efs","title":"EFS","text":"<ul> <li>Mountpoints can be in different AZs for redundancy</li> </ul>"},{"location":"studying/solutions_architect_professional/business_continuity/#ec2","title":"EC2","text":"<p>Reserved Instances is the only way to guarantee that resources will be available when needed.</p>"},{"location":"studying/solutions_architect_professional/business_continuity/#database-ha-options","title":"Database HA options","text":""},{"location":"studying/solutions_architect_professional/business_continuity/#dynamodb-ha","title":"DynamoDB HA","text":"<ul> <li>Distributes data across partitions by default</li> <li>Partitions are replicated synchronously across three AZs within a region</li> <li>Global Tables allow for multi-region availability and fault tolerance</li> </ul>"},{"location":"studying/solutions_architect_professional/business_continuity/#rds","title":"RDS","text":"<ul> <li>Multi-AZ should not be confused with read replica</li> <li>Multi-AZ is a replica of the database in another AZ but it cannot be read from</li> <li>Read replica - can have a replica in another region for regional disaster recovery (warm standby)</li> <li>Can use backup/restore with snapshots if the RPO is leniant and the RTO isn't immediate</li> </ul>"},{"location":"studying/solutions_architect_professional/business_continuity/#aurora","title":"Aurora","text":"<ul> <li>Has a main instance and can have multiple read replicas in separate AZs</li> <li>You can send read traffic to the read replicas</li> <li>During an AZ outage the read replica is promoted to the main instance</li> <li>Can have global databases which has one primary region and up to 5 secondary regions</li> </ul>"},{"location":"studying/solutions_architect_professional/business_continuity/#redshift","title":"Redshift","text":"<ul> <li>Datawarehouse availability</li> <li>Only RA3 instances support multi-az</li> <li>Best other option for HA is multi-node cluster</li> </ul>"},{"location":"studying/solutions_architect_professional/business_continuity/#network-ha","title":"Network HA","text":"<ul> <li>Use subnets in separate AZs within your VPC</li> <li>Direct connect is not highly available by default</li> <li>Use multiple tunnels in a VPN for redundancy</li> <li>Elastic IPs can allow changing out assets without impacting name resolution</li> <li>R53 health checks can provide DNS redirects during a failover</li> <li>NatGWs should be created in multiple AZs</li> </ul>"},{"location":"studying/solutions_architect_professional/business_continuity/#quiz","title":"Quiz","text":"<p>Which of the following is false about Redshift in the context of fault tolerance?</p> <ul> <li>Redshift multi-node clusters are Multi-AZ by default.</li> </ul> <p>Note</p> <p>Redshift (general availability) supports single-AZ deployments. A workaround would be to set up your data warehouse clusters in multiple AZ's by loading your data from the same S3 input files. Released as a preview feature in November 2022, Redshift can support Multi-AZ deployments for RA3 clusters, but it is not a default setting.</p> <p>Your client is seeking recommendations to help reduce the risk of underlying hardware failure on AWS. Which might you recommend?</p> <ul> <li>Make use of horizontal scaling over vertical scaling where possible</li> <li>Make use spread placement groups</li> </ul> <p>Note</p> <p>Purchasing reserved instances provides cost savings but does not directly address the risk of hardware failure.</p> <p>You need an in-memory cache, but you want it to be able to survive an AZ failure. Which option is best?</p> <ul> <li>Elasticache for Redis</li> </ul> <p>Note</p> <p>Elasticache for Redis supports multi-AZ configurations while Memcached does not.</p> <p>Your client has defined an RPO and RTO of 24 hours for their 2 GB database. What general approach would you recommend to fulfill these requirements most cost-effectively?</p> <ul> <li>Backup and Restore</li> </ul> <p>Note</p> <p>Pilot Light is a more complex and costly solution than necessary for a 24-hour RPO and RTO. It maintains a minimal version of the environment running with data replication, which can be scaled up when needed, but is still more costly than a simple backup and restore strategy. With the relatively small data size and generous RTO/RPO, a simple backup and restore process would work well. It is the most cost-effective approach for meeting the client's requirements and can be easily implemented.</p>"},{"location":"studying/solutions_architect_professional/cost_management/","title":"Cost management","text":""},{"location":"studying/solutions_architect_professional/cost_management/#cost-management","title":"Cost Management","text":""},{"location":"studying/solutions_architect_professional/cost_management/#design-solutions-for-organisational-complexity","title":"Design Solutions for Organisational Complexity","text":"<ul> <li>Cost and usage monitoring tools</li> <li>Purchasing options (savings plans, reserved instances, stpo instances)</li> <li>Rightsizing tools</li> </ul>"},{"location":"studying/solutions_architect_professional/cost_management/#design-for-new-solutions","title":"Design for new solutions","text":"<ul> <li>Storage tiering</li> <li>Data transfer costs</li> <li>Managed service options</li> </ul>"},{"location":"studying/solutions_architect_professional/cost_management/#continuous-improvement-for-existing-solutions","title":"Continuous Improvement for Existing Solutions","text":"<ul> <li>Network and data transfer costs</li> <li>Cost management, alerting and reporting</li> </ul>"},{"location":"studying/solutions_architect_professional/cost_management/#accelerate-workload-migration-and-modernisation","title":"Accelerate Workload Migration and Modernisation","text":"<ul> <li>Total cost of ownership</li> </ul>"},{"location":"studying/solutions_architect_professional/cost_management/#cost-management-overview","title":"Cost Management Overview","text":"<p>Capex - Capital Expenses - Money spent on long term assets like buildings and equipment Opex - Operational Expenses - Money spent on ongoing costs for running the business, usually variable expenses.</p> <p>Examples of CapEx include physical assets, such as buildings, equipment, machinery, and vehicles. Examples of OpEx include employee salaries, rent, utilities, and property taxes.</p>"},{"location":"studying/solutions_architect_professional/cost_management/#cost-optimisation-strategies","title":"Cost Optimisation Strategies","text":"<ul> <li>Appropriate provisioning</li> <li>Right-sizing</li> <li>Purchase options</li> <li>Geographic selection</li> <li>Managed services</li> <li>Optimised data transfer</li> </ul>"},{"location":"studying/solutions_architect_professional/cost_management/#appropriate-provisioning","title":"Appropriate provisioning","text":"<p>Provision only what you need, don't leave temporary instances running</p> <p>Consolidate where possible, perhaps one larger RDS instance than multiple smaller ones</p> <p>Cloudwatch monitor utilisation</p>"},{"location":"studying/solutions_architect_professional/cost_management/#rightsizing","title":"Rightsizing","text":"<p>Use lowest cost resource that still meets technical requirements</p>"},{"location":"studying/solutions_architect_professional/cost_management/#purchase-options","title":"Purchase options","text":"<p>Reserved instances provide the best cost advantage Spot instances are good for temporary horizontal scaling</p> <p>EC2 fleets is a mix of on demand, reserved and spot instances.</p>"},{"location":"studying/solutions_architect_professional/cost_management/#geographic-selection","title":"Geographic selection","text":"<p>AWS priving varies from region to region</p> <p>If it's cheaper in another region and doesn't need to be local then it might be worth hosting in another region</p>"},{"location":"studying/solutions_architect_professional/cost_management/#managed-services","title":"Managed services","text":"<p>Leverage RDS over database on EC2 for example</p> <p>Serverless, fargate means you don't need to manage or pay for the infrastructure</p>"},{"location":"studying/solutions_architect_professional/cost_management/#optimised-data-transfer","title":"Optimised data transfer","text":"<p>Moving data in to AWS doesn't cost anything, but moving data out or cross-region does.</p>"},{"location":"studying/solutions_architect_professional/cost_management/#resource-groups","title":"Resource groups","text":"<p>Are grouping of AWS assets defined by tags</p> <p>Can create custom consoles to consolidate metrics, alarm and config details around given tags</p> <p>EG - Dev, Test, Prod</p>"},{"location":"studying/solutions_architect_professional/cost_management/#cross-account","title":"Cross account","text":""},{"location":"studying/solutions_architect_professional/cost_management/#cost-and-usage-reports","title":"Cost and usage reports","text":"<p>Consolidated billing track spending across your organisation</p> <p>Generate CSV files to track costs Store the reports in S3 and analyse or visualise with Athena, redshift and quicksight.</p>"},{"location":"studying/solutions_architect_professional/cost_management/#centralised-budget-alerts","title":"Centralised budget alerts","text":"<p>Create a budget on a specific account or a number of them.</p>"},{"location":"studying/solutions_architect_professional/cost_management/#reserved-instances","title":"Reserved instances","text":"<ul> <li>Purchase or agree to purchase usage of EC2 instances in advance for a significant discount</li> <li>Provides guaranteed capacity reservation when used in a specific AZ</li> <li>Can be shared across multiple accounts with consolidated billing</li> <li>If you no longer need RIs you can try and sell them on the reserved instance marketplace</li> </ul> <p>Two types, can be Standard or Convertible</p> <p>Standard is a higher saving than Convertible</p> <p>For Convertible you can change the instance family, OS tenancy or payment options</p> <p>Convertible benefit from price redunctions</p> <p>Only standard is available to sell on the RI marketplace</p>"},{"location":"studying/solutions_architect_professional/cost_management/#spot-instances","title":"Spot instances","text":"<p>Customer creats a spot request and specifies AMI, instance type and other key information</p> <p>Dependant on EC2 capacity that AWS tries to sell on a market exchange basis</p> <p>Define the highest price you'll pay for the instance, it gets terminated it someone pays higher</p>"},{"location":"studying/solutions_architect_professional/cost_management/#dedicated-instances-and-hosts","title":"Dedicated Instances and Hosts","text":"<p>Dedicated Instances:</p> <ul> <li>Virtualised instances on hardware</li> <li>May share hardware with other non dedicated instances</li> <li>Available as On-demand, reserved and spot</li> <li>Cost an extra $2 per hour</li> </ul> <p>Dedicated Hosts:</p> <ul> <li>Physical servers dedicated to just your use</li> <li>You have control over which instances are deployed on that host</li> <li>Available as On-demand or Dedicated host reservation</li> <li>Useful if you have server-bound software licenses that use per-core, per-socker or per-vm</li> <li>Each dedicated host can only run one EC2 instance size and type</li> </ul>"},{"location":"studying/solutions_architect_professional/cost_management/#quiz","title":"Quiz","text":"<p>Which statement is true about Dedicated Instances and Dedicated Hosts?</p> <ul> <li>Dedicated Hosts reserve capacity.</li> <li>Dedicated Instances can run as spot instances.</li> </ul> <p>Note</p> <p>Unlike Reserved Instances, Dedicated Instances do not provide capacity reservation and merely ensure that your instance runs on hardware that's dedicated to a single customer. AWS allows for running Dedicated Instances as spot instances, providing a way to further reduce costs in exchange for the possibility that these instances can be terminated with short notice.</p>"},{"location":"studying/solutions_architect_professional/datastores/","title":"Datastores","text":""},{"location":"studying/solutions_architect_professional/datastores/#datastores","title":"Datastores","text":""},{"location":"studying/solutions_architect_professional/datastores/#design-solutions-for-organisational-complexity","title":"Design Solutions for Organisational Complexity","text":"<ul> <li>Deploying encryption strategies for data at rest and in transit</li> <li>Data backup and restoration</li> </ul>"},{"location":"studying/solutions_architect_professional/datastores/#design-for-new-solutions","title":"Design for new solutions","text":"<ul> <li>Storage options on AWS</li> <li>Storage tiering and transfer cost</li> <li>Configuring database replication</li> <li>Operating and maintaining high-availability</li> <li>AWS Storage services and replication strategies (S3/RDS/Elasticache)</li> </ul>"},{"location":"studying/solutions_architect_professional/datastores/#continuous-improvement-for-existing-solutions","title":"Continuous Improvement for Existing Solutions","text":"<ul> <li>Data retention, sensitivity and regulatory requirements</li> <li>Data replication methods</li> </ul>"},{"location":"studying/solutions_architect_professional/datastores/#accelerate-workload-migration-and-modernisation","title":"Accelerate Workload Migration and Modernisation","text":"<ul> <li>Different databases</li> <li>Selecting the appropriate storage service</li> <li>Selecting the appropriate database platform</li> </ul>"},{"location":"studying/solutions_architect_professional/datastores/#datastore-concepts","title":"Datastore Concepts","text":""},{"location":"studying/solutions_architect_professional/datastores/#s3","title":"S3","text":"<ul> <li>S3 is an object store</li> <li>Used in other AWS services behind the scenes</li> <li>Maximum object size is 5TB</li> <li>Largest object in a single PUT is 5GB</li> <li>Recommended to use multi-part uploads if larger than 100MB</li> </ul> <p>S3 has much more in common with a database than a filesystem</p> <p>Security options:</p> <ul> <li>Resource based - Object ACL and Bucket policy</li> <li>User-based - (IAM policies)</li> <li>Optional MFA before delete</li> </ul> <p>Storage classes:</p> <ul> <li>Standard</li> <li>Standard IA</li> <li>One-zone IA</li> <li>Reduced redundancy</li> <li>Intelligent tiering</li> <li>Glacier</li> <li>Glacier deep archive</li> </ul> <p>Encryption keys:</p> <ul> <li>SSE-S3 - S3s existing encryption key for AES-256</li> <li>SSE-C - Upload your own AWS-256 key</li> <li>SSE-KMS - Use a key generated by KMS</li> <li>Client-side - Encrypt objects using local encryption before uploading</li> </ul> <p>Bucket Polocies vs IAM:</p> <ul> <li>Bucket policies are defined at the bucket resource, you give access to all or some objects to a principal</li> <li>IAM policies grant temporary access in the form of a role to user, service or application</li> <li>Bucket policies are Resource-based</li> <li> <p>IAM policies are Identity-based</p> </li> <li> <p>First access is determined whether there is an explicit DENY on the Bucket Policy for said user</p> </li> <li>Then access is determined whether there is an ALLOW on the Bucket Policy for said user</li> <li>Then the IAM user permissions are checked whether there is a role they can assume with S3 permissions on the bucket</li> </ul> <p>Using an S3 Gateway Endpoint can be used to reduce cost for public Ingress/Egress to/from the S3 bucket.</p> <p>Glacier:</p> <ul> <li>Cheap but slow to respond and very infrequently accessed</li> <li>'Cold storage'</li> <li>Faster retrieval options if you pay</li> </ul> <p>Glacier Vault:</p> <ul> <li>Can use Glacier as a service without using S3</li> <li>Has Archives, similar to an S3 object</li> <li>Has Policies, defines what rules the vault must abide by (ie nobody can delete)</li> <li>Access to the Vault is administered by IAM</li> <li> <p>Archives and Policies can't be changed (immutable) but can be overwritten</p> </li> <li> <p>You create a Glacier Vault lock</p> </li> <li>You have 24 hours to confirm the vault lock</li> <li>If this isn't confirmed within 24 hours it is aborted</li> </ul>"},{"location":"studying/solutions_architect_professional/datastores/#ebs","title":"EBS","text":"<ul> <li>Similar to virtual hard drives</li> <li>Can only be assigned to EC2</li> <li>Tied to a single AZ</li> <li>Choices of IOPS, Throughput and Cost</li> </ul> <p>Instance stores:</p> <ul> <li>Temporary</li> <li>Only available when the EC2 instance is running</li> <li>Locked to the one instance</li> </ul> <p>Snapshots:</p> <ul> <li>Initial snapshot contains all data</li> <li>Subsequent snapshots only take a snapshot of the data added from the previous snapshot</li> </ul>"},{"location":"studying/solutions_architect_professional/datastores/#efs","title":"EFS","text":"<ul> <li>Implementation of NFS fileshare</li> <li>Elastic storage capacity, pay for what you use</li> <li>Multi-AZ</li> <li>Configure mount points in one or more AZs</li> <li>Can be mounted on-premise but network considerations are needed (Direct Connect recommended)</li> <li>Can use Datasync to sync on-premise data to EFS (or EFS to EFS)</li> <li>3 times more expensive to EBS and 10+ times more expensive to S3</li> </ul>"},{"location":"studying/solutions_architect_professional/datastores/#fsx","title":"FSx","text":"<p>FSx is a fileshare service is a distributed file system which provices options for non-NFS options for filesharing.</p> <p>Most commonly used for Windows file services as some windows applications may not work with EFS</p> <p>Four flavours:</p> <ul> <li>NetApp ONTAP</li> <li>OpenZFS</li> <li>FSx for windows file server</li> <li>FSx for Lustre</li> </ul> <p>Can use Managed Microsoft AD to provide access to FSx</p>"},{"location":"studying/solutions_architect_professional/datastores/#amazon-storage-gateway","title":"Amazon Storage Gateway","text":"<ul> <li>A VM that you can run on premise</li> <li>Provides local storage resources backed by S3 and Glacier</li> <li>Often used for DR to sync to AWS</li> <li> <p>Useful for cloud migrations getting data into AWS</p> </li> <li> <p>File gateway - NFS or SMB - Allow on prem or EC2 instances to store objects in S3 via NFS</p> </li> <li>Volume Gateway Stored - iSCSI - Async replication of on prep to S3</li> <li>Volume Gateway Cached - iSCSI - Primary data stored in S3 with frequently access data cached locally on-prem</li> <li>Tape gateway - iSCSI - Virtual media and tape library for use with existing backup software</li> </ul> <p>Use case would be initial migration to AWS from on-prem would use Volume Gateway Stored, then you can move to Volume Gateway Cached</p>"},{"location":"studying/solutions_architect_professional/datastores/#database-on-ec2","title":"Database on EC2","text":"<ul> <li>Run any DB on EC2 with full flexibility</li> <li>Have to manage backups, redundancy, patching</li> <li>Not a managed service</li> <li>Generally used when DB engines aren't supported in RDS</li> </ul>"},{"location":"studying/solutions_architect_professional/datastores/#rds","title":"RDS","text":"<ul> <li>Managed DB for MySQL, Maria, PostgreSQL, MSSQL, Oracle and MySQL Aurora</li> <li>Automated backup and patching in defined maint windows</li> <li>Push-button scaling, replication and redundancy</li> </ul> If you need Don't use RDS, use: Lots or large binary objects S3 Automated scalability DynamoDB Name/Value Data DynamoDB Data which isn't structured DynamoDB (NoSQL as there's no schema) Use a non RDS supported engine EC2 You need complete control of the DB engine EC2 <p>A multi-AZ deployment has a Master database in one AZ and a Standby (or Secondary) database in another AZ. Only the Master database serves traffic. If the Master fails, then the Secondary takes over.</p> <p>A Read Replica is a read-only copy of the database. It is actively running and apps can use it for read-only queries. A Read Replica can be in a different AZ or even in a different region.</p>"},{"location":"studying/solutions_architect_professional/datastores/#aurora","title":"Aurora","text":"<ul> <li>Fully managed RDS service for MysqL and PostreSQL</li> <li>Multi-AZ by design</li> <li>Autoscaling managed by AWS</li> <li>Easier multi-region replication</li> </ul> <p>Your main instance handles all write jobs, once data is written it's available to read in 100ms.</p> <p>Aurora is good for read traffic as you can distribute the read jobs across instances.</p> <p>Aurora natively handles high availability, if there's an AZ failure your read replica is moved to the master. The Cluster endpoint doesn't change.</p> <p>If you're reading from a read replica you can use the Reader endpoint.</p> <p>You can set up read replicas in different regions.</p> <p></p>"},{"location":"studying/solutions_architect_professional/datastores/#dynamodb","title":"DynamoDB","text":"<ul> <li>Managed Multi-az NoSQL datastore with Cross Region replication option</li> <li>Defaults to eventual consistency reads but can request strongly consistent reads via SDK parameter</li> <li>Priced on throughput rather than compute</li> <li>Provision read and write capacity in anticipation of need</li> <li>On demand capacity for flexible capacity at a small premium cost</li> <li>Achieve ACID compliance with DynamoDB Transactions</li> </ul> <p>Note</p> <p>In computer science, <code>ACID</code> is a set of properties of database transactions intended to guarantee data validity despite errors, power failures, and other mishaps. In the context of databases, a sequence of database operations that satisfies the ACID properties is called a transaction.</p> <p>For example, when a customer withdraws money from an ATM, an ACID transaction is executed to update their account balance and record the transaction. The transaction is atomic, meaning it succeeds or fails, and the account balance remains constant.</p> <p>A <code>table</code> is a collection of <code>items</code>, and each <code>item</code> is a collection of <code>attributes</code>. DynamoDB uses <code>primary keys</code> to uniquely identify each <code>item</code> in a <code>table</code> and <code>secondary indexes</code> to provide more querying flexibility.</p>"},{"location":"studying/solutions_architect_professional/datastores/#relational-vs-nosql","title":"Relational vs NoSQL","text":"<p>Relational concepts use tables and fienlds.</p> <p>NoSQL is designed to manage name value pairs ie JSON.</p>"},{"location":"studying/solutions_architect_professional/datastores/#read-and-write-capacity-units","title":"Read and Write capacity units","text":"<p>For on-demand mode tables, you don't need to specify how much read and write throughput you expect your application to perform. DynamoDB charges you for the reads and writes that your application performs on your tables in terms of read request units and write request units.</p> <p>DynamoDB <code>read requests</code> can be either strongly consistent, eventually consistent, or transactional.</p> <ul> <li>A strongly consistent read request of an item up to 4 KB requires one read request unit.</li> <li>An eventually consistent read request of an item up to 4 KB requires one-half read request unit.</li> <li>A transactional read request of an item up to 4 KB requires two read request units.</li> </ul> <p>If you need to read an item that is larger than 4 KB, DynamoDB needs additional read request units. The total number of read request units required depends on the item size, and whether you want an eventually consistent or strongly consistent read. For example, if your item size is 8 KB, you require 2 read request units to sustain one strongly consistent read, 1 read request unit if you choose eventually consistent reads, or 4 read request units for a transactional read request.</p> <p>One <code>write request</code> unit represents one write for an item up to 1 KB in size. If you need to write an item that is larger than 1 KB, DynamoDB needs to consume additional write request units. Transactional write requests require 2 write request units to perform one write for items up to 1 KB. The total number of write request units required depends on the item size. For example, if your item size is 2 KB, you require 2 write request units to sustain one write request or 4 write request units for a transactional write request.</p>"},{"location":"studying/solutions_architect_professional/datastores/#indexes-and-keys","title":"Indexes and keys","text":"<p>The below is a table named 'People'.</p> <pre><code>{\n    \"PersonID\": 101,\n    \"LastName\": \"Smith\",\n    \"FirstName\": \"Fred\",\n    \"Phone\": \"555-4321\"\n}\n\n{\n    \"PersonID\": 102,\n    \"LastName\": \"Jones\",\n    \"FirstName\": \"Mary\",\n    \"Address\": {\n                \"Street\": \"123 Main\",\n                \"City\": \"Anytown\",\n                \"State\": \"OH\",\n                \"ZIPCode\": 12345\n    }\n}\n\n{\n    \"PersonID\": 103,\n    \"LastName\": \"Stephens\",\n    \"FirstName\": \"Howard\",\n    \"Address\": {\n                \"Street\": \"123 Main\",\n                \"City\": \"London\",                                    \n                \"PostalCode\": \"ER3 5K8\"\n    },\n    \"FavoriteColor\": \"Blue\"\n}\n</code></pre> <ul> <li>Each item in the table has a unique identifier, or primary key, that distinguishes the item from all of the others in the table. In the People table, the <code>primary key</code> consists of one attribute (<code>PersonID</code>).</li> <li>Some of the items have a nested attribute (<code>Address</code>). DynamoDB supports nested attributes up to 32 levels deep.</li> </ul> <p>When you create a table, in addition to the table name, you must specify the <code>primary key</code> of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key.</p> <p>DynamoDB supports two different kinds of primary keys: - <code>Partition key</code> - A simple <code>primary key</code>, composed of one attribute known as the <code>partition key</code>. - <code>Partition key and sort key</code> \u2013 Referred to as a <code>composite primary key</code>, this type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key. A <code>composite primary</code> key gives you additional flexibility when querying data. For example, if you provide only the value for Artist, DynamoDB retrieves all of the songs by that artist. To retrieve only a subset of songs by a particular artist, you can provide a value for Artist along with a range of values for SongTitle.</p>"},{"location":"studying/solutions_architect_professional/datastores/#secondary-indexes","title":"Secondary indexes","text":"<p>You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn't require that you use indexes, but they give your applications more flexibility when querying your data. After you create a secondary index on a table, you can read data from the index in much the same way as you do from the table.</p> <p>DynamoDB supports two kinds of indexes:</p> <ul> <li><code>Global secondary index</code> \u2013 An <code>index</code> with a <code>partition key</code> and <code>sort key</code> that can be different from those on the table.</li> <li><code>Local secondary index</code> \u2013 An <code>index</code> that has the same <code>partition key</code> as the table, but a different <code>sort key</code>.</li> </ul> <p>The following diagram shows the example <code>Music</code> table, with a new index called <code>GenreAlbumTitle</code>. In the index, <code>Genre</code> is the <code>partition key</code> and <code>AlbumTitle</code> is the <code>sort key</code>.</p> Music Table GenreAlbumTitle <pre>{  \"Artist\": \"No One You Know\",  \"SongTitle\": \"My Dog Spot\",  \"AlbumTitle\": \"Hey Now\",  \"Price\": 1.98,  \"Genre\": \"Country\",  \"CriticRating\": 8.4}</pre> <pre>{    \"Genre\": \"Country\",    \"AlbumTitle\": \"Hey Now\",    \"Artist\": \"No One You Know\",    \"SongTitle\": \"My Dog Spot\"}</pre> <pre>{    \"Artist\": \"No One You Know\",    \"SongTitle\": \"Somewhere Down The Road\",    \"AlbumTitle\": \"Somewhat Famous\",    \"Genre\": \"Country\",    \"CriticRating\": 8.4,    \"Year\": 1984}</pre> <pre>{    \"Genre\": \"Country\",    \"AlbumTitle\": \"Somewhat Famous\",    \"Artist\": \"No One You Know\",    \"SongTitle\": \"Somewhere Down The Road\"}</pre> <pre>{    \"Artist\": \"The Acme Band\",    \"SongTitle\": \"Still in Love\",    \"AlbumTitle\": \"The Buck Starts Here\",    \"Price\": 2.47,    \"Genre\": \"Rock\",    \"PromotionInfo\": {        \"RadioStationsPlaying\": {            \"KHCR\",            \"KQBX\",            \"WTNR\",            \"WJJH\"        },        \"TourDates\": {            \"Seattle\": \"20150622\",            \"Cleveland\": \"20150630\"        },        \"Rotation\": \"Heavy\"    }}</pre> <pre>{    \"Genre\": \"Rock\",    \"AlbumTitle\": \"The Buck Starts Here\",    \"Artist\": \"The Acme Band\",    \"SongTitle\": \"Still In Love\"}</pre> <pre>{    \"Artist\": \"The Acme Band\",    \"SongTitle\": \"Look Out, World\",    \"AlbumTitle\": \"The Buck Starts Here\",    \"Price\": 0.99,    \"Genre\": \"Rock\"}</pre> <pre>{    \"Genre\": \"Rock\",    \"AlbumTitle\": \"The Buck Starts Here\",    \"Artist\": \"The Acme Band\",    \"SongTitle\": \"Look Out, World\"}</pre> Index type Description Remember Global Secondary Index Partition key and sort key can be different from those on the table Not restriced to just the partitioning set forth by the partition key, global Local Secondary Index Same partition key as the table but different sort key Have to stay local and respect the tables partition key but can choose whatever sort key I want Index type When to use Example Global Secondary Index When you want a fast query of attributes outside of the primary key \"I'd like to query sales orders by customer number rather than sales order number\" Local Secondary Index When you already know the partition key and want to quickly query on another attribute \"I have the sales order number but I'd like to retrieve only those records with a certain Material number\" <p>You can query the GenreAlbumTitle index to find all albums of a particular genre (for example, all Rock albums). You can also query the index to find all albums within a particular genre that have certain album titles (for example, all Country albums with titles that start with the letter H).</p> <pre><code>{\n    \"salesordernum\" : \"12346435\",\n    \"timestamp\" : \"22-10-2023\",\n    \"salesorder\" : {\n        \"salesordertytpe\" : \"schedule\",\n        \"materialnum\" : \"123217856\"\n    },\n    \"customer\" : {\n        \"customernum\" : \"234235\",\n        \"customername\" : \"Jimbob\"\n    }\n}\n</code></pre> <p>If you created a Global Secondary Index using <code>customernum</code> you could query by Customer Number at light-speed.</p> <p>If you created a Local Secondary Index using <code>materialnum</code> you could query bu Sales Order Number and Material Number at light-speed.</p> <p>Attribute projections are attributes projected into an index, ie:</p> <ul> <li>customernum (key)</li> <li>customername</li> <li>salesordernum</li> <li>timestamp</li> <li>materialnum</li> </ul> <p>You can also create a <code>replica</code> table by creating a <code>Global secondary index</code> using the same <code>partition key</code> and <code>sort key</code>. A use case for this would be if you wanted the same set of data present to higher and lower tier customers, you can set higher RCU/WCU limits for the higher tier customers.</p> <p></p>"},{"location":"studying/solutions_architect_professional/datastores/#documentdb","title":"DocumentDB","text":"<p>DocumentDB is a NoSQL JSON document database service, it is designed to be compatibale with MongoDB.</p> <p>DocumentDB has one main instance and multiple replicas, similar to Aurora.</p>"},{"location":"studying/solutions_architect_professional/datastores/#redshift","title":"Redshift","text":"<p>Fully managed clustered peta-byte scale data wahrehouse. It's PostresSQL compatible.</p>"},{"location":"studying/solutions_architect_professional/datastores/#preparing-for-data","title":"Preparing for Data","text":"<p>ETL</p> <ul> <li>Extract</li> <li>Transform</li> <li>Load</li> </ul> <p><code>Extract</code> only the data relevant to your data need.</p> <p><code>Transform</code> the data ie sorting, de-duplicating.</p> <p><code>Load</code> the data into your application source.</p>"},{"location":"studying/solutions_architect_professional/datastores/#aws-glue","title":"AWS Glue","text":"<p>ETL at scale. Glue crawlers can pull data.</p> <p>AWS Glue is serverless.</p> <p>Data sources: S3, Redshift, RDS or DB on EC2.</p> <p>A Glue Crawler will collect data and stores data in a Glue Data Catalog.</p> <p>You can then use something like Athena to parse that data.</p>"},{"location":"studying/solutions_architect_professional/datastores/#aws-athena","title":"AWS Athena","text":"<p>AWS Athena is a managed service. You can use it to analyse data where it lives.</p> <p>Can ingest databases from S3 objects and use standard SQL to query that data.</p>"},{"location":"studying/solutions_architect_professional/datastores/#aws-neptune","title":"AWS Neptune","text":"<p>Fully managed graph database which supports open graph APIs for both Gremlin and SPARQL.</p>"},{"location":"studying/solutions_architect_professional/datastores/#elasticache","title":"Elasticache","text":"<p>Fully managed implementation of in-memory datastores - Redis and Memcached</p> <p>Billed by node size and hours of use.</p> <p>Use cases:</p> <ul> <li>Storing web sessions</li> <li>Database caching - taking load off the RDS layer</li> <li>Leaderboards</li> </ul> <p><code>Memcached</code> is generally the no-frills straight forward option for when you need to cache objects.</p> <p><code>Redis</code> is more feature heavy, supports clustering, backup/restore and you need encryption to run it.</p>"},{"location":"studying/solutions_architect_professional/datastores/#other-database-options","title":"Other Database options","text":"<ul> <li><code>Athena</code> - used to query raw data in S3 via SQL.</li> <li><code>Quantum Ledger</code> - based on blockchain, provides immutable journal as a service.</li> <li><code>Timestream</code> - Keeping up with time-series data</li> <li><code>Opensearch</code> - Mostly a search engine but also a document store, used to be Elasticsearch</li> </ul>"},{"location":"studying/solutions_architect_professional/datastores/#quiz","title":"Quiz","text":"<p>What DynamoDB features can be utilised to increase the speed of read operations?</p> <ul> <li>DynamoDB Accelerator (DAX)</li> <li>Secondary Indexes</li> </ul> <p>Note</p> <p>DynamoDB Accelerator (DAX) works as an in-memory cache in front of DynamoDB, which can accelerate many read operations. Secondary indexes contain a subset of attributes from a table and an alternate key for queries. These can help speed up certain read operations.</p> <p>Which of the following options allows users to have secure access to private files located in S3?</p> <ul> <li>CloudFront signed cookies</li> <li>CloudFront signed URLs</li> <li>CloudFront origin access identity</li> </ul> <p>Note</p> <p>Signed cookies generate special cookies to grant secure access to objects in an S3 bucket, but they require the creation of an application and policy to generate and control these items. Signed URLs generate special URLs to grant secure access to objects in an S3 bucket, but require the creation of an application and policy to generate and control these items. An origin access identity is a virtual user identity that is used to give the CloudFront distribution permission to fetch a private object from an S3 bucket.</p> <p>Which of the following are true statements regarding Amazon Athena?</p> <ul> <li>Athena requires no servers to be set up or managed.</li> <li>Amazon Athena provides an easy way to analyze data in Amazon S3 using standard SQL.</li> <li>Athena can run ad-hoc queries using ANSI SQL.</li> </ul> <p>You are tasked with choosing a storage service for a high-performance computing (HPC) environment based on Amazon Linux. The workload stores and processes a large number of videos that require shared, parallel storage and compute-intensive transcoding. Which of the following storage options would be the optimal solution?</p> <ul> <li>FSx for Lustre</li> </ul> <p>Note</p> <p>FSx for Lustre is optimized for HPC environments and provides parallel storage for files being processed by HPC clusters.</p> <p>__ is an immutable way to set policies on a Glacier vault, such as retention or enforcing MFA before delete.</p> <ul> <li>Glacier Vault Lock</li> </ul> <p>Which database service would be appropriate for an application that requires a flexible, JSON-formatted schema?</p> <ul> <li>DocumentDB</li> <li>S3 and Athena</li> </ul> <p>Note</p> <p>DocumentDB is a document-store database that keeps data in JSON-formatted documents. These documents have flexible schemas and are designed to be intuitively structured such that developers can easily make sense of the data contained in each document. JSON objects can be stored in S3 and queried with Amazon Athena.</p> <p>Which of the following data formats does Amazon Athena support?</p> <ul> <li>Apache ORC data</li> <li>JSON data</li> <li>Apache Parquet data</li> </ul> <p>You need low-latency access to your entire dataset. Amazon S3 should be used to back up your on-premises data periodically, so you can restore it in the event of a disaster in your data center. Which AWS Storage Gateway solution offers the most suitable features for this use case?</p> <ul> <li>Volume Gateway - Stored Volumes</li> </ul> <p>Note</p> <p>Stored volumes provide low-latency access to entire datasets, where on-premises gateways are configured to store all data locally. Point-in-time snapshots from the local data are taken asynchronously to Amazon S3. This configuration provides durable and inexpensive offsite backups that can be recovered to local data centers or Amazon EC2.</p> <p>Which of the following are considered attributes of the ACID compliance model?</p> <ul> <li>The ACID consistency model is Atomic, Consistent, Isolated, and Durable.</li> </ul> <p>Note</p> <p>Consistent transactions must be valid. Atomic transactions are \"all or nothing\". Durable means that a completed transaction must stick around. Isolated transactions can't mess with one another.</p> <p>You are architecting a complex application landscape that values fast disk I/O for EC2 instances above everything else. Which storage option would you choose?</p> <ul> <li>Instance Store</li> </ul> <p>Note</p> <p>EBS can provide very high I/O for your EC2 instances, but the locally attached Instance Store drives provide the highest I/O potential. Instance Store, because it is locally attached, provides the fastest disk I/O among the choices. Note the question did not specify the need for the storage to be persistent, so ephemeral options are in play too.</p> <p>You need to improve the performance of queries to your DynamoDB table. The most common queries do not use the partition key. What should you do?</p> <ul> <li>Create a global secondary index with the most common queried attribute as the partition key.</li> </ul> <p>Note</p> <p>A global secondary index can be used to speed up queries against non-primary key items. They have their own throughput capacity separate to the table. Using the most common queried attribute as the global secondary index's partition key lets us query on it directly.</p>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/","title":"Deployment and operations management","text":""},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#deployment-and-operations-management","title":"Deployment and Operations Management","text":""},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#design-for-new-solutions","title":"Design for new solutions","text":"<ul> <li>IAC</li> <li>CI/CD</li> <li>Config management (systems manager)</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#continuous-improvement-for-existing-solutions","title":"Continuous Improvement for Existing Solutions","text":"<ul> <li>Improve CI/CD pipeline</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#accelerate-workload-migration-and-modernisation","title":"Accelerate Workload Migration and Modernisation","text":"<ul> <li>Portfolio assessment</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#deployment-overview","title":"Deployment Overview","text":""},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#rolling-deployment","title":"Rolling deployment","text":"<ul> <li>Create a new launch template version with a new AMI</li> <li>Terminate EC2 instances and have them update to the new AMI</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#ab-testing","title":"A/B Testing","text":"<ul> <li>Use Route53 to send 10% of the traffic to the new version of code</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#canary-release","title":"Canary release","text":"<ul> <li>Deploy version 2 of code on one EC2 instance</li> <li>If this has no issues then deploy to the rest of the instances</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#blue-green","title":"Blue Green","text":"<ul> <li>Have two environments running</li> <li>Switch over using Route 53 to the Green version</li> <li>If there's issues then switch back to the Blue version</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#blue-green-issues","title":"Blue Green issues","text":"<ul> <li>Hard to use blue/green on the web layer if the code updates are coupled with a Data layer update</li> <li>Some upgrades require upgrade routines, licensing etc</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#elastic-beanstalk","title":"Elastic Beanstalk","text":"<ul> <li>Orchestration service to make push button deployments</li> <li>Wide range of supported platforms from Docker, PHP, Java to Node.js</li> <li>Supports multiple environments within the app ie Dev, QA, Prod</li> <li>Great for ease of deployment but not great if you require maximum control</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#cloudformation","title":"Cloudformation","text":""},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#stack-policies","title":"Stack policies","text":"<p>Protect specific resources from your stack from being unintentionally deleted or updated</p> <p>Example:</p> <pre><code>{\n  \"Statement\" : [\n    {\n      \"Effect\" : \"Allow\",\n      \"Action\" : \"Update:*\",\n      \"Principal\": \"*\",\n      \"Resource\" : \"*\"\n    },\n    {\n      \"Effect\" : \"Deny\",\n      \"Action\" : \"Update:*\",\n      \"Principal\": \"*\",\n      \"Resource\" : \"LogicalResourceId/ProductionDatabase\"\n    }\n  ]\n}\n</code></pre>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#stacksets","title":"Stacksets","text":"<p>Deploy and update resources across your organisation</p>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#service-catalog","title":"Service Catalog","text":"<p>Create products and share those products with other accounts in the organisation to allow users to deploy resources.</p> <p>Portfolios can contain many products</p>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#api-gateway","title":"API Gateway","text":"<ul> <li>Managed high available service to front-end REST APIs</li> <li>Backed with custom code via Lambda, as a proxy for another AWS service or any HTTP API</li> <li>Can be:<ul> <li>Regionally based</li> <li>Private</li> <li>Edge Optimised</li> </ul> </li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#management-tools","title":"Management tools","text":""},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#aws-config","title":"AWS Config","text":"<ul> <li>Allows you to assess, audit and evaluate configurations of your AWS resources</li> <li>Very useful for config management as part of an ITIL program</li> <li>Creates a baseline of various config settings and files and then tracks variations against that baseline</li> <li>Config rules can check resources for compliancy</li> </ul> <p>Example of config rules:</p> <ul> <li>Is backup enabled on RDS?</li> <li>Do EC2 instances have required tags?</li> <li>Are EBS volumes encrypted?</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#opsworks","title":"Opsworks","text":"<ul> <li>Managed instance of Chef and Puppet</li> <li>Provide config management to deploy code, automate tasks, configure instances, perform upgrades</li> <li>Three offerings:<ul> <li>Opsworks for Chef automate</li> <li>Opsworks for Puppet enterprise</li> <li>Opsworks stacks</li> </ul> </li> <li>Opsworks for Chef automate - fully managed implementation of Chef</li> <li>Opsworks for Puppet enterprise - fully managed implementation of Puppet</li> <li>Opsworks Stacks - AWS creation and uses an embedded Chef solo client installed on EC2 instances to run Chef recipes. Also supports on-prem</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#systems-manager","title":"Systems Manager","text":"<ul> <li>Centralised console and toolset for a wide variety of system management tasks</li> <li>Requires SSM agent</li> <li>Designed for managing large fleet of systems, tens or hundreds</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#business-applications-and-end-user-computing","title":"Business Applications and End-user computing","text":"<ul> <li>Amazon appstream - Similar to workspaces but only apps eg Word, Excel</li> <li>AWS Client VPN -</li> <li>Amazon Chime - Meeting and video conferencing (like skype and zoom)</li> <li>Alexa for business - deploy alexa functionality and skills internally like echo in conference room</li> <li>Amazon worklink - Secure access to internal web/applications - basically a secure proxy</li> <li>Amazon workspaces - Desktop as a service</li> <li>AWS connect - Contact center - telephony</li> <li>Amazon workdocs - online document storage (like google docs)</li> <li>Amazon workmail - managed email and calendar as a service, compatible with exchange</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#machine-learning","title":"Machine Learning","text":"<ul> <li>Amazon comprehend - Natural languace processing - finds insight and relationships within text</li> <li>Amazon polly - text to speech</li> <li>Amazon translate - translate text to different languages</li> <li>Amazon forecast - combines time-series data to deliver accurate forecasts</li> <li>Amazon lex - conversational interface - chatbox</li> <li>Amazon personalise - recommendation enginer based on demographic behavioural data</li> <li>Amazon rekognition - image and video analysis</li> <li>Amazon textract - extract text from scanned documents</li> <li>Amazon transcribe - speech to text</li> </ul> <p>Amazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment.</p>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#iot","title":"IOT","text":"<p>The Internet of Things (IoT) describes the network of physical objects\u2014\u201cthings\u201d\u2014that are embedded with sensors, software, and other technologies for the purpose of connecting and exchanging data with other devices and systems over the internet.</p> <p>Device connection:</p> <ul> <li>AWS IOT Core - route IOT messages by the trillions</li> <li>AWS IOT 1-click - simple integration for compatible devices</li> <li>AWS IOT Events - trigger alerts when events occur</li> <li>AWS IOT Greengrass - Build IOT device software</li> <li>AWS IOT Things graph - develop workflows and models for integrated devices</li> </ul> <p>Device Management:</p> <ul> <li>AWS IOT Device management - Device registry, organising devices, firmware updates</li> <li>AWS IOT Device defender - security service for IOT configurations</li> </ul> <p>Analytics and visiblity:</p> <ul> <li>AWS IOT Analytics - Develop reports from IOT devices</li> <li>AWS IOT Sitewise - Monitor IOT devices on the edge</li> </ul>"},{"location":"studying/solutions_architect_professional/deployment_and_operations_management/#quiz","title":"Quiz","text":"<p>Creating a new AMI, assigning it to a new Launch Configuration, and then updating an existing Auto Scaling Group is an example of which upgrade method?</p> <ul> <li>Disposable upgrade</li> </ul> <p>Note</p> <p>A disposable upgrade is one where a new release is deployed on new instances while instances containing the old version are terminated. A multi-stage upgrade involves deploying the new version in stages, gradually replacing the older version. While the given scenario does involve replacing instances, it does not explicitly mention a staged approach.</p> <p>You are helping a customer build a CloudFormation template. During the stack creation, you need to get a software license key from a third party via API call. What resource would you use?</p> <ul> <li>AWS::CloudFormation::CustomResource</li> </ul> <p>Note</p> <p>There are a few ways to call an API but among these choices, AWS:CloudFormation::CustomResource is the only one that can be used to call out to fetch a value from a third-party API.</p> <p>How can you allow users to provision a standard API Gateway and Lambda microservice in their AWS Account in your organization without giving them permissions to the underlying services?</p> <ul> <li>Use AWS Service Catalog to create a product with a CloudFormation template that deploys the standard API Gateway and Lambda microservice, and grant users access to this product.</li> </ul> <p>You are responsible for managing AWS cloud solutions in multiple regions. You want to use OpsWorks Stacks to help in that management. Which of the following are false?</p> <ul> <li>OpsWorks allows you to clone stacks within the same region and to other regions.</li> </ul> <p>Note</p> <p>While OpsWorks does allow you to clone stacks within the same region, it does not support cloning stacks to other regions directly. You would need to manually recreate the stack in the desired region.</p> <p>You have been collecting IoT messages from hundreds of vibration and temperature monitoring IoT devices in your warehouse. These devices are connected to AWS IoT Core. You want to build reports based on the time-series data collected from these devices. Which solution should you implement to achieve this goal?</p> <ul> <li>Send the data from IoT Core to AWS IoT Analytics, process the data, and use Amazon QuickSight to build reports based on the time-series data.</li> </ul> <p>Note</p> <p>Sending data from AWS IoT Core to AWS IoT Analytics allows you to process and analyze the IoT data. You can then use Amazon QuickSight to visualize and create reports based on the processed time-series data.</p>"},{"location":"studying/solutions_architect_professional/migrations/","title":"Migrations","text":""},{"location":"studying/solutions_architect_professional/migrations/#migrations","title":"Migrations","text":""},{"location":"studying/solutions_architect_professional/migrations/#design-solutions-for-organisational-complexity","title":"Design Solutions for Organisational Complexity","text":"<ul> <li>Data backup and restoration</li> </ul>"},{"location":"studying/solutions_architect_professional/migrations/#design-for-new-solutions","title":"Design for new solutions","text":"<ul> <li>AWS Storage services (S3, RDS, Elasticache)</li> <li>Data transfer costs</li> </ul>"},{"location":"studying/solutions_architect_professional/migrations/#continuous-improvement-for-existing-solutions","title":"Continuous Improvement for Existing Solutions","text":"<ul> <li>Backup practices and methods</li> <li>Data replication</li> </ul>"},{"location":"studying/solutions_architect_professional/migrations/#accelerate-workload-migration-and-modernisation","title":"Accelerate Workload Migration and Modernisation","text":"<ul> <li>Migration assessment and tools</li> <li>Application migration tools (AWS Application discovery service, application migration service)</li> </ul>"},{"location":"studying/solutions_architect_professional/migrations/#migration-hub","title":"Migration Hub","text":"<ul> <li>Leverages <code>Application discovery service</code> to analyse and groups servers</li> <li>Gives instance recommendations to help right-size</li> <li>Provides situational migration strategy best practice</li> <li>Keeps track of lift and shift</li> </ul>"},{"location":"studying/solutions_architect_professional/migrations/#application-migration-service-mgn","title":"Application Migration Service (MGN)","text":"<ul> <li>Migrates windows and linux servers to AWS cloud</li> <li>Source can be on-prem, AWS or other cloud providers</li> <li>Allows for test environment</li> </ul> <p>Can be used to migrate from one region to another.</p> <p></p>"},{"location":"studying/solutions_architect_professional/migrations/#database-migration-service","title":"Database migration service","text":"<ul> <li>Used to migrate source relational databases to RDS, Aurora or EC2 instance</li> </ul>"},{"location":"studying/solutions_architect_professional/migrations/#aws-transfer-family","title":"AWS Transfer Family","text":"<p>Transfer files to and from EFS</p> <ul> <li>Multi-AZ for HA</li> <li>Uses SFTP, FTP, FTPS and AS2 for file protocols</li> </ul>"},{"location":"studying/solutions_architect_professional/migrations/#quiz","title":"Quiz","text":"<p>You have decided to migrate your on-premises legacy Informix database to Amazon Aurora. How might this be facilitated most efficiently?</p> <ul> <li>Manually create the target schema on Aurora and then use Data Pipeline with JDBC to move the data.</li> </ul> <p>Note</p> <p>Informix is not supported by either the Data Migration Service or Schema Conversion Tool, so the only choice among these options is manually creating the schema and using Data Pipeline with JDBC to move the data.</p> <p>Which of the following are valid use cases for AWS Migration Hub?</p> <ul> <li>Consolidating and visualizing migration progress in a single dashboard</li> <li>Tracking the progress of multiple migrations across AWS services</li> </ul> <p>Note</p> <p>AWS Migration Hub provides a single dashboard to consolidate and visualize migration progress, making it easier to monitor and manage multiple migrations.</p> <p></p> <p>You are evaluating a technical migration plan for a customer. Which of the following project assumptions is incorrect?</p> <ul> <li>We can use AWS Server Migration Service to replicate Linux, Windows, and Solaris VMs, syncing volumes and creating periodic AMIs</li> </ul> <p>Note</p> <p>Sun Solaris is not a supported OS for AWS, and thus not able to be migrated using AWS SMS.</p> <p>What is the key difference between AWS DataSync and AWS Storage Gateway?</p> <ul> <li>AWS DataSync is used for online data transfer between on-premises storage systems and AWS, while Storage Gateway is a hybrid cloud storage service that integrates on-premises environments with cloud storage.</li> </ul> <p>Note</p> <p>AWS DataSync focuses on transferring data between on-premises storage systems and AWS, whereas AWS Storage Gateway provides a hybrid cloud storage solution that enables on-premises applications to seamlessly use AWS cloud storage.</p> <p>Which of the following steps are necessary when migrating an application using the AWS Application Migration Service?</p> <ul> <li>Configure the target infrastructure using the Application Migration Service Console.</li> <li>Install the AWS Replication Agent on the source servers.</li> <li>Perform a non-disruptive test to validate the migration.</li> </ul> <p>Note</p> <p>Configuring the target infrastructure is a necessary step when using the Application Migration Service, as it sets up the required resources in the target environment. Performing a non-disruptive test is a critical step in the migration process, as it validates the migration and ensures that the application will function correctly in the target environment. Installing the AWS Replication Agent on the source servers is a required step, as it enables communication between the source servers and the Application Migration Service.</p> <p>You are migrating from an Oracle on-premises database to an Oracle RDS database. Which of these describes this migration properly?</p> <ul> <li>Homogeneous migration</li> </ul> <p>Note</p> <p>A homogeneous migration is when we migrate between the same type of databases.</p>"},{"location":"studying/solutions_architect_professional/networking/","title":"Networking","text":""},{"location":"studying/solutions_architect_professional/networking/#networking","title":"Networking","text":""},{"location":"studying/solutions_architect_professional/networking/#design-solutions-for-organisational-complexity","title":"Design Solutions for Organisational Complexity","text":"<ul> <li>AWS VPC, VPN, TGW</li> <li>Hybrid DNS, Route53 resolver</li> <li>Network segmentation</li> <li>Network traffic monitoring</li> <li>Route tables, SG, NACLs</li> </ul>"},{"location":"studying/solutions_architect_professional/networking/#design-for-new-solutions","title":"Design for new solutions","text":"<ul> <li>Rotue53</li> <li>AWS service endpoints</li> <li>Multi-AZ and multi-region</li> </ul>"},{"location":"studying/solutions_architect_professional/networking/#continuous-improvement-for-existing-solutions","title":"Continuous Improvement for Existing Solutions","text":"<ul> <li>HA and resilient network</li> <li>AWS Global network offerings if global accelerator, cloudfront and edge computing</li> </ul>"},{"location":"studying/solutions_architect_professional/networking/#accelerate-workload-migration-and-modernisation","title":"Accelerate Workload Migration and Modernisation","text":"<ul> <li>Hybrid network, direct connect, S2S VPN</li> </ul>"},{"location":"studying/solutions_architect_professional/networking/#networking-concepts","title":"Networking Concepts","text":"<p>AWS Reserved IP addresses, eg VPC subnet CIDR is 10.0.0.0/24</p> <ul> <li>10.0.0.0 - Network Address</li> <li>10.0.0.1 - Reserved for the VPC router</li> <li>10.0.0.2 - Reserved for AWS DNS</li> <li>10.0.0.3 - Reserved for future use</li> <li>10.0.0.255 - VPCs don't support broadcast so AWS reserves this address</li> </ul>"},{"location":"studying/solutions_architect_professional/networking/#vpc-to-vpc-networking","title":"VPC to VPC networking","text":"<ul> <li>VPC Peering Direct traffic between VPCs using the AWS network</li> <li>AWS Transit Gateway - can be used to direct traffic across many VPCs</li> <li>Software VPN</li> <li>AWS Managed VPN</li> </ul> <p>VPC Peering has no Transitive Peering.</p> <p>ie  <code>VPC A &lt;---&gt; VPC B &lt;---&gt; VPC C</code></p> <p>VPC A cannot talk to VPC C.</p> <p>Transit Gateway needs an TGW Attachment in a subnet in each AZ.</p> <p></p> <p>TGW peering within the same region can use TGW attachments across accounts, as long as they are within region.</p> <p>TGW Peering is needed when communicating across Regions.</p> <p></p>"},{"location":"studying/solutions_architect_professional/networking/#internet-gateway","title":"Internet Gateway","text":"<p>No availability risk as it's completely redundant. There are also no bandwidth constraints.</p> <p>Provides route table target for Internet bound traffic</p> <p>Also performs NAT on instances with public IP addresses.</p> <p>Does not perform NAT for instances with private IPs only.</p> <p>Egress-only internet gateway prevents inbound connectivity to IPv6 instances.</p>"},{"location":"studying/solutions_architect_professional/networking/#border-gateway-protocol","title":"Border Gateway Protocol","text":"<ul> <li>Required for Direct connect and optional for VPN</li> <li>Alternative of not using BGP with AWS VPC is static routes</li> <li>AWS supports BGP community tagging as a way to control traffic scope and route preference</li> <li>Requires TCP port 179</li> </ul>"},{"location":"studying/solutions_architect_professional/networking/#placement-groups","title":"Placement groups","text":"Clustered Spread Partition What Instances placed into a low-latency group within an AZ Instances spread across underlying Hardware across different AZs Instances grouped into partitions and spread across racks When Low Latency / High network throughput Hardware redundancy Hardware redundancy Pros Enhanced networking Can span AZs Better than spread for large replicated workloads Cons Finite capacity Max 7 instances per group per AZ Doesn't support dedicated hosts <p>The default placement is to attempt to spread the instances.</p>"},{"location":"studying/solutions_architect_professional/networking/#direct-connect","title":"Direct Connect","text":"<ul> <li>Connects your Datacenter to AWS global network</li> <li>Reduces dependency on public ISP demand</li> <li>Works with ISPs to establish the Direct connect connection</li> <li>Great for frequent large transfers of data</li> </ul>"},{"location":"studying/solutions_architect_professional/networking/#high-availability","title":"High Availability","text":"<p>You can use a backup S2S VPN in case the Direct Connect connection goes down.</p> <p>You can also set up two Direct Connect connections using two different ISPs for redundancy.</p> <p>Lastly you can use Direct Connect SITELINK to establish connectivity between two Datacenters via AWS. This is useful should the internet go down, you can still access the DCs from eachother using the AWS network.</p> <p></p>"},{"location":"studying/solutions_architect_professional/networking/#privatelink","title":"PrivateLink","text":"<ul> <li>Securely connect to VPC endpoints</li> <li>Highly available and scalable</li> <li>Traffic does not traverse the public internet</li> <li>Control API endpoints, sites and services reachable from your VPC</li> <li>Can only connect within region</li> </ul>"},{"location":"studying/solutions_architect_professional/networking/#global-accelerator","title":"Global Accelerator","text":"<ul> <li>Allows public app users to shortcut past public infrastructure using AWS network at edge locations</li> <li>Provides global static public IPs to access your application endpoints</li> <li>Can use used for failover in multi-region atchitecture</li> <li>Your static entry points are protected by AWS shield</li> </ul>"},{"location":"studying/solutions_architect_professional/networking/#route-53","title":"Route 53","text":"Failover Policy Route53 Simple Simple failover Failover Fails over to the backup if the Primary fails a health-check Geolocation Routes based on the region Geoproximity Routes based on the region taking into consideration region ie 'us-east-1' over 'us-west-2' Latency Directs to resources based on the lwest latency Multi-value answer Similar to a load balancer, R53 responds with multiple addresses Weighted You can setup mulitple resources and routes based on percentage of weight assigned"},{"location":"studying/solutions_architect_professional/networking/#cloudfront","title":"Cloudfront","text":"<p>Is a content delivery service for simple static catching</p>"},{"location":"studying/solutions_architect_professional/networking/#quiz","title":"Quiz","text":"<p>You have an app currently hosted in three Regions around the globe and you have defined Route 53 geolocation routing to route people to the nearest Region. Some customers complain that they are not able to access the service. What could be the cause?</p> <ul> <li>You need to ensure that you have a default route in addition to other geolocation routes.</li> </ul> <p>Note</p> <p>For geolocation routing, you need to be sure you have a default record in the case that the location cannot be determined. This will help ensure all users can access the service, even if their location cannot be identified.</p> <p>Which of the following DNS record types does Route 53 not support?</p> <ul> <li>TLSA</li> </ul> <p>Note</p> <p>TLSA records are used to specify the keys used in a domain's TLS servers. This record type is not supported by Amazon Route 53 at the moment.</p> <p>You have a domain name registered with Route53 in Account A. You would like to register a subdomain, or a child domain, in Account B. You have already created the Hosted Zone for the subdomain in Account B. What is the next step to allow for proper query forwarding from the parent DNS account to the new subdomain?</p> <ul> <li>Copy the NS record from the newly created hosted zone in Account B. Create a new NS record in the Hosted Zone for the parent domain in Account A and paste the NS records for the subdomain.</li> </ul> <p>Note</p> <p>Creating and linking the NS records between the parent domain and subdomain allows for proper query forwarding between the two.</p> <p>Which AWS networking component is used for IPv6 traffic only?</p> <ul> <li>Egress-only internet gateway</li> </ul> <p>Note</p> <p>An egress-only internet gateway is for use with IPv6 traffic only.</p> <p>What might I consider to decrease the likelihood that multiple EC2 instances are impacted by some sort of underlying hardware failure in AWS?</p> <ul> <li>Spread placement groups</li> </ul> <p>Note</p> <p>Spread Placement Groups ensure that your instances are placed on physically separate hosts, reducing the risk of multiple instances being impacted by a single hardware failure.</p> <p>You are building an application hosted on AWS for a customer. The customer has a very old firewall that can whitelist external destinations via IP address only. Which solution could you use to allow your AWS application to leverage static IP addresses that can be whitelisted by the customer firewall?</p> <ul> <li>Application loadbalancer with Global accelerator</li> <li>Network loadbalancer with EIP</li> </ul> <p>Note</p> <p>You can use AWS Global Accelerator to get static IP addresses that act as a fixed entry point to your application endpoints. Network Load Balancers support static IP addresses. You can also assign one Elastic IP address per subnet enabled for the load balancer. Reference: What is a Network Load Balancer?.</p> <p>Which of these statements on Direct Connect are FALSE?</p> <ul> <li>AWS Direct Connect can be connected to an internet gateway to route traffic to the internet.</li> <li>Direct Connect connections are highly available.</li> </ul> <p>Note</p> <p>Direct Connect connections consist of a single connection between your network and AWS with no inherent redundancy. Additionally, traffic coming from on-premises via a Direct Connect connect is restricted from internet access.</p> <p>You want to allow your VPC instances to resolve using on-premises DNS. Can you do this and how/why?</p> <ul> <li>Yes, by configuring a DHCP Option Set to issue your on-premises DNS IP to VPC clients.</li> </ul> <p>Note</p> <p>You can use DHCP Option Sets to configure which DNS server addresses are configured for EC2 instances in a VPC. This can be any IP address. As long as its can be reached from the VPC, instances can use it to resolve DNS queries.</p>"},{"location":"studying/solutions_architect_professional/security/","title":"Security","text":""},{"location":"studying/solutions_architect_professional/security/#security","title":"Security","text":""},{"location":"studying/solutions_architect_professional/security/#design-solutions-for-organisational-complexity","title":"Design Solutions for Organisational Complexity","text":"<ul> <li>Network monitoring</li> <li>IAM / SSO</li> <li>Encryption keys and ACM</li> <li>Security - Cloudtrail, IAM Access analyser, Securityhub</li> <li>RAM</li> </ul>"},{"location":"studying/solutions_architect_professional/security/#design-for-new-solutions","title":"Design for new solutions","text":"<ul> <li>IAM</li> <li>Encryption for data at rest and in transit</li> <li>Credential management</li> <li>AWS Managed services - WAF, Shield, GuardDuty</li> </ul>"},{"location":"studying/solutions_architect_professional/security/#continuous-improvement-for-existing-solutions","title":"Continuous Improvement for Existing Solutions","text":"<ul> <li>Data retention</li> <li>Secrets management</li> <li>Principle of least privilege</li> <li>Monitoring - cloudwatch</li> </ul>"},{"location":"studying/solutions_architect_professional/security/#accelerate-workload-migration-and-modernisation","title":"Accelerate Workload Migration and Modernisation","text":"<ul> <li>Governance - Organisations / control tower</li> </ul>"},{"location":"studying/solutions_architect_professional/security/#security-concepts","title":"Security Concepts","text":"<p>Shared responsibility model</p> <p></p> <p>Least privileges:</p> <ul> <li>Give users and services nothing more than those privileges necessary</li> <li>Only when they need them, not long term</li> </ul>"},{"location":"studying/solutions_architect_professional/security/#organisations","title":"Organisations","text":"<p>Good to split Dev / Prod accounts as if one set of keys get accessed it's only within the one environment.</p> <p>Consolodated billing with organisations allows for bulk discounts across the organisation rather than account level.</p>"},{"location":"studying/solutions_architect_professional/security/#control-tower","title":"Control Tower","text":"<p><code>Landing Zone</code> - What you provision when you start using Control Tower, your landing zone is a recommended, customisable starting point.</p> <p><code>Guardrail</code> - High level rules governed by SCPs or Config rules</p> <p><code>Baseline</code> - Combination of blueprints (CFN stacks) and guardrailes applied to a member account.</p> <p>Following accounts created:</p> <ul> <li>Log Archive - where logs are stored</li> <li>Audit - Cross account audit roles</li> </ul>"},{"location":"studying/solutions_architect_professional/security/#service-control-policies","title":"Service Control Policies","text":"<p>IAM syntax, only <code>DENY</code> access, never allow.</p> <p>Best practice is to only apply SCPs to OUs, not accounts.</p> <p>A <code>DENY</code> list explicitly denies specific actions</p> <p>An <code>ALLOW</code> list denies all actions that are NOT listed. Orgs are assigned FullAWSAccess by default.</p>"},{"location":"studying/solutions_architect_professional/security/#config","title":"Config","text":"<p>Monitors best practices across Organisation</p> <p>Defines detective controls and shows which accounts are compliant. Keeps a history of actions that caused non-compliance.</p>"},{"location":"studying/solutions_architect_professional/security/#iam-identity-center","title":"IAM Identity Center","text":"<p>Formally AWS SSO.</p> <p>Uses roles instead of IAM users.</p> <p>Maps users/groups from an identity provider to IAM users and groups</p> <p>It integrates with identity providers that leverege SAML 2.0 (like Azure AD)</p> <p>Can act as a user directory.</p> <p></p>"},{"location":"studying/solutions_architect_professional/security/#directory-services","title":"Directory Services","text":"Service Description Best For: Cloud Directory Cloud-native directory to share controll access between applications Cloud applications that need hierarchical data Cognito Sign up and sign-in functionality that scales to millions of users Consumer apps AWS Directory Service for Microsoft AD Full Microsoft AD (Standard or enterprise) Hosted AD for LADP AD Connector Allows on-prem users to log into AWS services with AD SSO for on prep epmloyees and domain joined EC2 instances Simple AD Low scale and low cost AD implementation based on Samba Simple user directory"},{"location":"studying/solutions_architect_professional/security/#secrets-manager","title":"Secrets manager","text":"<ul> <li>Automatically rotates RDS database credentials for MySQL, PostgreSQL and Aurora</li> </ul> <p>Applications need to use the AWS SDK to access the secret 'getSecretValue'.</p>"},{"location":"studying/solutions_architect_professional/security/#resource-access-manager","title":"Resource access manager","text":"<ul> <li>Share resources with principals across many accounts</li> <li>Share with accounts or OUs in an organisation</li> </ul> <p>Note</p> <p>Resource sharing MUST be enabled in your organisation</p> <p>You can share a subnet from one account to another. The target account cannot delete the subnet.</p>"},{"location":"studying/solutions_architect_professional/security/#cloud-hsm","title":"Cloud HSM","text":"<ul> <li>Dedicated hardware device, owned only by you</li> <li>Must be within a VPC, can be accessed via peering</li> <li>Doesn't integrate with many services like KMS but rather requires custom application scripting</li> <li>Offload SSL from web servers, act as a certificate authority</li> </ul>"},{"location":"studying/solutions_architect_professional/security/#acm","title":"ACM","text":"<ul> <li>Managed service to provision, manage and deploy public/private SSL/TLS certificates</li> <li>Supports wildcard (*.domain.com)</li> <li>Managed certificate renewal</li> </ul>"},{"location":"studying/solutions_architect_professional/security/#network-firewall-vs-waf","title":"Network Firewall vs WAF","text":"<p>Network firewall assumes traffic is coming into your VPC and filters traffic to and from that VPC.</p> <p>WAF works with ALB, Cloudfront, API Gateway, Appsync and more</p>"},{"location":"studying/solutions_architect_professional/security/#shield","title":"Shield","text":"<p>Primary service for DDOS mitigation</p> <p></p>"},{"location":"studying/solutions_architect_professional/security/#guard-duty","title":"Guard Duty","text":"<p>Uses machine learning and log inspection to generate security alerts and recommendations.</p> <p>Threat detection with low operational overhead.</p>"}]}