## Datastores

### Design Solutions for Organisational Complexity

- Deploying encryption strategies for data at rest and in transit
- Data backup and restoration

### Design for new solutions

- Storage options on AWS
- Storage tiering and transfer cost
- Configuring database replication
- Operating and maintaining high-availability
- AWS Storage services and replication strategies (S3/RDS/Elasticache)

### Continuous Improvement for Existing Solutions

- Data retention, sensitivity and regulatory requirements
- Data replication methods

### Accelerate Workload Migration and Modernisation

- Different databases
- Selecting the appropriate storage service
- Selecting the appropriate database platform

## Datastore Concepts

![datastore_concepts](../../assets/images/datastore_concepts.png "datastore_concepts.png")

![iops_throughput](../../assets/images/iops_throughput.png "iops_throughput.png")

## S3

- S3 is an object store
- Used in other AWS services behind the scenes
- Maximum object size is 5TB
- Largest object in a single PUT is 5GB
- Recommended to use multi-part uploads if larger than 100MB

S3 has much more in common with a database than a filesystem

Security options:

- Resource based - Object ACL and Bucket policy
- User-based - (IAM policies)
- Optional MFA before delete

Storage classes:

- Standard
- Standard IA
- One-zone IA
- Reduced redundancy
- Intelligent tiering
- Glacier
- Glacier deep archive

Encryption keys:

- SSE-S3 - S3s existing encryption key for AES-256
- SSE-C - Upload your own AWS-256 key
- SSE-KMS - Use a key generated by KMS
- Client-side - Encrypt objects using local encryption before uploading

Bucket Polocies vs IAM:

- Bucket policies are defined at the bucket resource, you give access to all or some objects to a principal
- IAM policies grant temporary access in the form of a role to user, service or application
- Bucket policies are Resource-based
- IAM policies are Identity-based

- First access is determined whether there is an explicit DENY on the Bucket Policy for said user
- Then access is determined whether there is an ALLOW on the Bucket Policy for said user
- Then the IAM user permissions are checked whether there is a role they can assume with S3 permissions on the bucket

Using an S3 Gateway Endpoint can be used to reduce cost for public Ingress/Egress to/from the S3 bucket.

Glacier:

- Cheap but slow to respond and very infrequently accessed
- 'Cold storage'
- Faster retrieval options if you pay

Glacier Vault:

- Can use Glacier as a service without using S3
- Has Archives, similar to an S3 object
- Has Policies, defines what rules the vault must abide by (ie nobody can delete)
- Access to the Vault is administered by IAM
- Archives and Policies can't be changed (immutable) but can be overwritten

- You create a Glacier Vault lock
- You have 24 hours to confirm the vault lock
- If this isn't confirmed within 24 hours it is aborted

## EBS

- Similar to virtual hard drives
- Can only be assigned to EC2
- Tied to a single AZ
- Choices of IOPS, Throughput and Cost

Instance stores:

- Temporary
- Only available when the EC2 instance is running
- Locked to the one instance

Snapshots:

- Initial snapshot contains all data
- Subsequent snapshots only take a snapshot of the data added from the previous snapshot

## EFS

- Implementation of NFS fileshare
- Elastic storage capacity, pay for what you use
- Multi-AZ
- Configure mount points in one or more AZs
- Can be mounted on-premise but network considerations are needed (Direct Connect recommended)
- Can use Datasync to sync on-premise data to EFS (or EFS to EFS)
- 3 times more expensive to EBS and 10+ times more expensive to S3

## FSx

FSx is a fileshare service is a distributed file system which provices options for non-NFS options for filesharing.

Most commonly used for Windows file services as some windows applications may not work with EFS

Four flavours:

- NetApp ONTAP
- OpenZFS
- FSx for windows file server
- FSx for Lustre

Can use Managed Microsoft AD to provide access to FSx

## Amazon Storage Gateway

- A VM that you can run on premise
- Provides local storage resources backed by S3 and Glacier
- Often used for DR to sync to AWS
- Useful for cloud migrations getting data into AWS

- File gateway - NFS or SMB - Allow on prem or EC2 instances to store objects in S3 via NFS
- Volume Gateway Stored - iSCSI - Async replication of on prep to S3
- Volume Gateway Cached - iSCSI - Primary data stored in S3 with frequently access data cached locally on-prem
- Tape gateway - iSCSI - Virtual media and tape library for use with existing backup software

Use case would be initial migration to AWS from on-prem would use Volume Gateway Stored, then you can move to Volume Gateway Cached

## Database on EC2

- Run any DB on EC2 with full flexibility
- Have to manage backups, redundancy, patching
- Not a managed service
- Generally used when DB engines aren't supported in RDS

## RDS

- Managed DB for MySQL, Maria, PostgreSQL, MSSQL, Oracle and MySQL Aurora
- Automated backup and patching in defined maint windows
- Push-button scaling, replication and redundancy

| If you need | Don't use RDS, use: |
| ----------- | ------------------- |
| Lots or large binary objects | S3 |
| Automated scalability | DynamoDB |
| Name/Value Data | DynamoDB |
| Data which isn't structured | DynamoDB (NoSQL as there's no schema) |
| Use a non RDS supported engine | EC2 |
| You need complete control of the DB engine | EC2 |

A multi-AZ deployment has a Master database in one AZ and a Standby (or Secondary) database in another AZ. Only the Master database serves traffic. If the Master fails, then the Secondary takes over.

A Read Replica is a read-only copy of the database. It is actively running and apps can use it for read-only queries. A Read Replica can be in a different AZ or even in a different region.

## Aurora

- Fully managed RDS service for MysqL and PostreSQL
- Multi-AZ by design
- Autoscaling managed by AWS
- Easier multi-region replication

![aurora1](../../assets/images/aurora1.png "aurora1.png")

Your main instance handles all write jobs, once data is written it's available to read in 100ms.

Aurora is good for read traffic as you can distribute the read jobs across instances.

Aurora natively handles high availability, if there's an AZ failure your read replica is moved to the master. The Cluster endpoint doesn't change.

If you're reading from a read replica you can use the Reader endpoint.

You can set up read replicas in different regions.

![aurora2](../../assets/images/aurora2.png "aurora2.png")
